{
  "full_suite": {
    "success": false,
    "returncode": 1,
    "stdout": "============================= test session starts ==============================\nplatform linux -- Python 3.11.0rc1, pytest-8.4.1, pluggy-1.6.0 -- /usr/bin/python3.11\ncachedir: .pytest_cache\nrootdir: /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp\nconfigfile: pytest.ini\nplugins: mock-3.15.1, xdist-3.8.0, cov-7.0.0, asyncio-1.1.0\nasyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollecting ... collected 491 items\n\ntests/edge_cases/test_edge_cases.py::TestZeroAndNegativeValues::test_zero_image_dimensions PASSED [  0%]\ntests/edge_cases/test_edge_cases.py::TestZeroAndNegativeValues::test_negative_coordinates PASSED [  0%]\ntests/edge_cases/test_edge_cases.py::TestZeroAndNegativeValues::test_negative_numeric_validation PASSED [  0%]\ntests/edge_cases/test_edge_cases.py::TestZeroAndNegativeValues::test_zero_file_size PASSED [  0%]\ntests/edge_cases/test_edge_cases.py::TestZeroAndNegativeValues::test_negative_timeout_values PASSED [  1%]\ntests/edge_cases/test_edge_cases.py::TestZeroAndNegativeValues::test_zero_tile_count_scenarios PASSED [  1%]\ntests/edge_cases/test_edge_cases.py::TestEmptyAndNullInputs::test_empty_string_inputs PASSED [  1%]\ntests/edge_cases/test_edge_cases.py::TestEmptyAndNullInputs::test_none_value_handling PASSED [  1%]\ntests/edge_cases/test_edge_cases.py::TestEmptyAndNullInputs::test_empty_collections PASSED [  1%]\ntests/edge_cases/test_edge_cases.py::TestEmptyAndNullInputs::test_empty_file_handling PASSED [  2%]\ntests/edge_cases/test_edge_cases.py::TestEmptyAndNullInputs::test_missing_optional_parameters PASSED [  2%]\ntests/edge_cases/test_edge_cases.py::TestUnicodeAndSpecialCharacters::test_unicode_file_paths PASSED [  2%]\ntests/edge_cases/test_edge_cases.py::TestUnicodeAndSpecialCharacters::test_unicode_text_in_validation PASSED [  2%]\ntests/edge_cases/test_edge_cases.py::TestUnicodeAndSpecialCharacters::test_special_characters_in_paths PASSED [  2%]\ntests/edge_cases/test_edge_cases.py::TestUnicodeAndSpecialCharacters::test_unicode_error_messages PASSED [  3%]\ntests/edge_cases/test_edge_cases.py::TestUnicodeAndSpecialCharacters::test_long_unicode_strings PASSED [  3%]\ntests/edge_cases/test_edge_cases.py::TestPlatformSpecificBehaviors::test_path_separator_handling PASSED [  3%]\ntests/edge_cases/test_edge_cases.py::TestPlatformSpecificBehaviors::test_case_sensitivity_handling PASSED [  3%]\ntests/edge_cases/test_edge_cases.py::TestPlatformSpecificBehaviors::test_long_path_handling PASSED [  3%]\ntests/edge_cases/test_edge_cases.py::TestPlatformSpecificBehaviors::test_unix_specific_features PASSED [  4%]\ntests/edge_cases/test_edge_cases.py::TestPlatformSpecificBehaviors::test_windows_specific_features SKIPPED [  4%]\ntests/edge_cases/test_edge_cases.py::TestResourceExhaustion::test_memory_pressure_simulation PASSED [  4%]\ntests/edge_cases/test_edge_cases.py::TestResourceExhaustion::test_file_descriptor_exhaustion PASSED [  4%]\ntests/edge_cases/test_edge_cases.py::TestResourceExhaustion::test_disk_space_simulation PASSED [  4%]\ntests/edge_cases/test_edge_cases.py::TestResourceExhaustion::test_concurrent_resource_contention PASSED [  5%]\ntests/edge_cases/test_edge_cases.py::TestCorruptedDataHandling::test_corrupted_image_files PASSED [  5%]\ntests/edge_cases/test_edge_cases.py::TestCorruptedDataHandling::test_malformed_configuration_data FAILED [  5%]\ntests/edge_cases/test_edge_cases.py::TestCorruptedDataHandling::test_invalid_coordinate_data FAILED [  5%]\ntests/edge_cases/test_edge_cases.py::TestCorruptedDataHandling::test_truncated_file_handling PASSED [  5%]\ntests/edge_cases/test_edge_cases.py::TestCorruptedDataHandling::test_invalid_metadata_handling PASSED [  6%]\ntests/edge_cases/test_edge_cases.py::TestExtremeInputValues::test_extremely_large_numbers FAILED [  6%]\ntests/edge_cases/test_edge_cases.py::TestExtremeInputValues::test_floating_point_precision PASSED [  6%]\ntests/edge_cases/test_edge_cases.py::TestExtremeInputValues::test_boundary_value_analysis PASSED [  6%]\ntests/edge_cases/test_edge_cases.py::TestExtremeInputValues::test_string_length_extremes PASSED [  6%]\ntests/edge_cases/test_edge_cases.py::TestExtremeInputValues::test_coordinate_extremes PASSED [  7%]\ntests/edge_cases/test_edge_cases.py::TestRaceConditionsAndTiming::test_concurrent_file_access PASSED [  7%]\ntests/edge_cases/test_edge_cases.py::TestRaceConditionsAndTiming::test_timing_dependent_operations PASSED [  7%]\ntests/edge_cases/test_edge_cases.py::TestRaceConditionsAndTiming::test_resource_cleanup_timing PASSED [  7%]\ntests/integration/test_cli.py::TestCLIBasics::test_cli_app_creation PASSED [  7%]\ntests/integration/test_cli.py::TestCLIBasics::test_cli_help_command PASSED [  8%]\ntests/integration/test_cli.py::TestCLIBasics::test_cli_version_command PASSED [  8%]\ntests/integration/test_cli.py::TestCLIBasics::test_cli_no_args_shows_help FAILED [  8%]\ntests/integration/test_cli.py::TestCLIBasics::test_cli_hello_command PASSED [  8%]\ntests/integration/test_cli.py::TestCLIConfiguration::test_config_file_auto_detection PASSED [  8%]\ntests/integration/test_cli.py::TestCLIConfiguration::test_explicit_config_file PASSED [  9%]\ntests/integration/test_cli.py::TestCLIConfiguration::test_config_file_not_found PASSED [  9%]\ntests/integration/test_cli.py::TestCLIConfiguration::test_verbose_flag PASSED [  9%]\ntests/integration/test_cli.py::TestCLIConfiguration::test_quiet_flag PASSED [  9%]\ntests/integration/test_cli.py::TestCLIConfiguration::test_verbose_and_quiet_conflict PASSED [  9%]\ntests/integration/test_cli.py::TestCLIConfiguration::test_global_state_initialization FAILED [ 10%]\ntests/integration/test_cli.py::TestCLIConfiguration::test_context_object_creation PASSED [ 10%]\ntests/integration/test_cli.py::TestCLICommands::test_install_completion_command PASSED [ 10%]\ntests/integration/test_cli.py::TestCLICommands::test_install_completion_show_path PASSED [ 10%]\ntests/integration/test_cli.py::TestCLICommands::test_install_completion_specific_shell PASSED [ 10%]\ntests/integration/test_cli.py::TestCLICommands::test_error_test_command PASSED [ 11%]\ntests/integration/test_cli.py::TestCLIErrorHandling::test_handle_retileup_error FAILED [ 11%]\ntests/integration/test_cli.py::TestCLIErrorHandling::test_handle_keyboard_interrupt PASSED [ 11%]\ntests/integration/test_cli.py::TestCLIErrorHandling::test_handle_keyboard_interrupt_quiet_mode PASSED [ 11%]\ntests/integration/test_cli.py::TestCLIErrorHandling::test_handle_typer_exit PASSED [ 12%]\ntests/integration/test_cli.py::TestCLIErrorHandling::test_handle_typer_abort PASSED [ 12%]\ntests/integration/test_cli.py::TestCLIErrorHandling::test_handle_generic_exception PASSED [ 12%]\ntests/integration/test_cli.py::TestCLIErrorHandling::test_handle_generic_exception_verbose PASSED [ 12%]\ntests/integration/test_cli.py::TestCLIErrorHandling::test_retileup_error_quiet_mode PASSED [ 12%]\ntests/integration/test_cli.py::TestCLIIntegration::test_cli_with_missing_commands PASSED [ 13%]\ntests/integration/test_cli.py::TestCLIIntegration::test_cli_context_propagation PASSED [ 13%]\ntests/integration/test_cli.py::TestCLIIntegration::test_cli_callback_order FAILED [ 13%]\ntests/integration/test_cli.py::TestCLIIntegration::test_cli_rich_output_integration PASSED [ 13%]\ntests/integration/test_cli.py::TestCLIIntegration::test_main_execution_path FAILED [ 13%]\ntests/integration/test_cli.py::TestCLIPerformance::test_cli_startup_time PASSED [ 14%]\ntests/integration/test_cli.py::TestCLIPerformance::test_cli_memory_usage PASSED [ 14%]\ntests/integration/test_cli.py::TestCLIPerformance::test_cli_concurrent_execution FAILED [ 14%]\ntests/integration/test_cli.py::TestCLIEdgeCases::test_cli_with_unicode_arguments PASSED [ 14%]\ntests/integration/test_cli.py::TestCLIEdgeCases::test_cli_with_very_long_arguments PASSED [ 14%]\ntests/integration/test_cli.py::TestCLIEdgeCases::test_cli_with_special_characters PASSED [ 15%]\ntests/integration/test_cli.py::TestCLIEdgeCases::test_cli_environment_isolation PASSED [ 15%]\ntests/integration/test_cli.py::TestCLIEdgeCases::test_cli_signal_handling FAILED [ 15%]\ntests/integration/test_cli.py::TestCLIEdgeCases::test_cli_with_corrupted_config PASSED [ 15%]\ntests/integration/test_cli.py::TestCLIEdgeCases::test_cli_permission_denied_config PASSED [ 15%]\ntests/integration/test_cli.py::TestCLIEdgeCases::test_cli_with_empty_config_file PASSED [ 16%]\ntests/integration/test_cli.py::TestCLIEdgeCases::test_cli_callback_exception_handling PASSED [ 16%]\ntests/integration/test_end_to_end.py::TestEndToEndCLI::test_cli_help_displays_correctly PASSED [ 16%]\ntests/integration/test_end_to_end.py::TestEndToEndCLI::test_cli_version_displays_correctly PASSED [ 16%]\ntests/integration/test_end_to_end.py::TestEndToEndCLI::test_cli_with_config_file PASSED [ 16%]\ntests/integration/test_end_to_end.py::TestEndToEndCLI::test_cli_error_handling PASSED [ 17%]\ntests/integration/test_end_to_end.py::TestEndToEndCLI::test_cli_global_options_propagation PASSED [ 17%]\ntests/integration/test_end_to_end.py::TestEndToEndCLI::test_cli_completion_installation PASSED [ 17%]\ntests/integration/test_end_to_end.py::TestEndToEndImageProcessing::test_basic_image_tiling SKIPPED [ 17%]\ntests/integration/test_end_to_end.py::TestEndToEndImageProcessing::test_workflow_execution SKIPPED [ 17%]\ntests/integration/test_end_to_end.py::TestEndToEndImageProcessing::test_image_validation_workflow PASSED [ 18%]\ntests/integration/test_end_to_end.py::TestEndToEndImageProcessing::test_image_format_conversion_workflow PASSED [ 18%]\ntests/integration/test_end_to_end.py::TestEndToEndImageProcessing::test_image_processing_memory_efficiency FAILED [ 18%]\ntests/integration/test_end_to_end.py::TestEndToEndConfiguration::test_config_file_loading PASSED [ 18%]\ntests/integration/test_end_to_end.py::TestEndToEndConfiguration::test_config_validation_workflow PASSED [ 18%]\ntests/integration/test_end_to_end.py::TestEndToEndConfiguration::test_config_override_workflow PASSED [ 19%]\ntests/integration/test_end_to_end.py::TestEndToEndConfiguration::test_config_validation_command SKIPPED [ 19%]\ntests/integration/test_end_to_end.py::TestEndToEndErrorScenarios::test_invalid_input_file_workflow PASSED [ 19%]\ntests/integration/test_end_to_end.py::TestEndToEndErrorScenarios::test_permission_denied_workflow PASSED [ 19%]\ntests/integration/test_end_to_end.py::TestEndToEndErrorScenarios::test_insufficient_disk_space_simulation PASSED [ 19%]\ntests/integration/test_end_to_end.py::TestEndToEndErrorScenarios::test_corrupted_config_file_workflow PASSED [ 20%]\ntests/integration/test_end_to_end.py::TestEndToEndErrorScenarios::test_network_timeout_simulation PASSED [ 20%]\ntests/integration/test_end_to_end.py::TestEndToEndPerformance::test_cli_startup_performance PASSED [ 20%]\ntests/integration/test_end_to_end.py::TestEndToEndPerformance::test_large_image_handling PASSED [ 20%]\ntests/integration/test_end_to_end.py::TestEndToEndPerformance::test_memory_usage_monitoring PASSED [ 20%]\ntests/integration/test_end_to_end.py::TestEndToEndPerformance::test_concurrent_processing_capability PASSED [ 21%]\ntests/integration/test_end_to_end.py::TestEndToEndCompatibility::test_path_handling_compatibility PASSED [ 21%]\ntests/integration/test_end_to_end.py::TestEndToEndCompatibility::test_image_format_compatibility PASSED [ 21%]\ntests/integration/test_end_to_end.py::TestEndToEndCompatibility::test_unicode_path_handling PASSED [ 21%]\ntests/integration/test_end_to_end.py::TestEndToEndCompatibility::test_environment_variable_handling PASSED [ 21%]\ntests/integration/test_end_to_end.py::TestEndToEndRealWorldScenarios::test_batch_processing_simulation PASSED [ 22%]\ntests/integration/test_end_to_end.py::TestEndToEndRealWorldScenarios::test_workflow_state_persistence PASSED [ 22%]\ntests/integration/test_end_to_end.py::TestEndToEndRealWorldScenarios::test_error_recovery_workflow PASSED [ 22%]\ntests/integration/test_end_to_end.py::TestEndToEndRealWorldScenarios::test_production_readiness_checklist PASSED [ 22%]\ntests/integration/test_tiling_integration.py::TestTilingIntegration::test_end_to_end_workflow PASSED [ 23%]\ntests/integration/test_tiling_integration.py::TestTilingIntegration::test_large_image_tiling FAILED [ 23%]\ntests/integration/test_tiling_integration.py::TestTilingIntegration::test_memory_limit_handling PASSED [ 23%]\ntests/integration/test_tiling_integration.py::TestTilingIntegration::test_edge_case_handling PASSED [ 23%]\ntests/integration/test_tiling_integration.py::TestTilingIntegration::test_format_preservation PASSED [ 23%]\ntests/integration/test_tiling_integration.py::TestTilingIntegration::test_concurrent_safety FAILED [ 24%]\ntests/integration/test_tiling_integration.py::TestTilingIntegration::test_error_recovery_and_partial_results PASSED [ 24%]\ntests/integration/test_workflows.py::TestWorkflowBasics::test_workflow_tool_registration ERROR [ 24%]\ntests/integration/test_workflows.py::TestWorkflowBasics::test_workflow_tool_retrieval ERROR [ 24%]\ntests/integration/test_workflows.py::TestWorkflowBasics::test_workflow_config_validation PASSED [ 24%]\ntests/integration/test_workflows.py::TestWorkflowBasics::test_workflow_config_validation_errors PASSED [ 25%]\ntests/integration/test_workflows.py::TestWorkflowExecution::test_simple_workflow_execution ERROR [ 25%]\ntests/integration/test_workflows.py::TestWorkflowExecution::test_workflow_with_output_files ERROR [ 25%]\ntests/integration/test_workflows.py::TestWorkflowExecution::test_workflow_execution_failure ERROR [ 25%]\ntests/integration/test_workflows.py::TestWorkflowExecution::test_workflow_execution_with_timing ERROR [ 25%]\ntests/integration/test_workflows.py::TestWorkflowExecution::test_workflow_multiple_executions ERROR [ 26%]\ntests/integration/test_workflows.py::TestWorkflowExecution::test_workflow_parallel_configuration ERROR [ 26%]\ntests/integration/test_workflows.py::TestWorkflowConcurrency::test_concurrent_workflow_execution ERROR [ 26%]\ntests/integration/test_workflows.py::TestWorkflowConcurrency::test_workflow_thread_safety ERROR [ 26%]\ntests/integration/test_workflows.py::TestWorkflowConcurrency::test_workflow_resource_contention ERROR [ 26%]\ntests/integration/test_workflows.py::TestWorkflowErrorHandling::test_workflow_validation_error_handling ERROR [ 27%]\ntests/integration/test_workflows.py::TestWorkflowErrorHandling::test_workflow_processing_error_recovery ERROR [ 27%]\ntests/integration/test_workflows.py::TestWorkflowErrorHandling::test_workflow_timeout_handling ERROR [ 27%]\ntests/integration/test_workflows.py::TestWorkflowErrorHandling::test_workflow_partial_failure_handling ERROR [ 27%]\ntests/integration/test_workflows.py::TestWorkflowErrorHandling::test_workflow_cleanup_on_failure ERROR [ 27%]\ntests/integration/test_workflows.py::TestWorkflowPerformance::test_workflow_execution_time ERROR [ 28%]\ntests/integration/test_workflows.py::TestWorkflowPerformance::test_workflow_memory_efficiency ERROR [ 28%]\ntests/integration/test_workflows.py::TestWorkflowPerformance::test_workflow_scalability ERROR [ 28%]\ntests/integration/test_workflows.py::TestWorkflowPerformance::test_workflow_resource_cleanup ERROR [ 28%]\ntests/integration/test_workflows.py::TestWorkflowIntegration::test_end_to_end_workflow ERROR [ 28%]\ntests/integration/test_workflows.py::TestWorkflowIntegration::test_workflow_chaining ERROR [ 29%]\ntests/integration/test_workflows.py::TestWorkflowIntegration::test_workflow_error_propagation ERROR [ 29%]\ntests/integration/test_workflows.py::TestWorkflowIntegration::test_workflow_configuration_inheritance ERROR [ 29%]\ntests/integration/test_workflows.py::TestWorkflowIntegration::test_workflow_state_management ERROR [ 29%]\ntests/performance/test_performance.py::TestImageProcessingPerformance::test_image_loading_performance PASSED [ 29%]\ntests/performance/test_performance.py::TestImageProcessingPerformance::test_image_resize_performance PASSED [ 30%]\ntests/performance/test_performance.py::TestImageProcessingPerformance::test_image_format_conversion_performance PASSED [ 30%]\ntests/performance/test_performance.py::TestImageProcessingPerformance::test_batch_image_processing_performance PASSED [ 30%]\ntests/performance/test_performance.py::TestImageProcessingPerformance::test_memory_efficiency_repeated_operations PASSED [ 30%]\ntests/performance/test_performance.py::TestImageProcessingPerformance::test_large_image_handling FAILED [ 30%]\ntests/performance/test_performance.py::TestValidationPerformance::test_file_path_validation_performance PASSED [ 31%]\ntests/performance/test_performance.py::TestValidationPerformance::test_batch_validation_performance PASSED [ 31%]\ntests/performance/test_performance.py::TestValidationPerformance::test_coordinate_validation_performance PASSED [ 31%]\ntests/performance/test_performance.py::TestConcurrencyPerformance::test_concurrent_image_loading PASSED [ 31%]\ntests/performance/test_performance.py::TestConcurrencyPerformance::test_concurrent_validation PASSED [ 31%]\ntests/performance/test_performance.py::TestConcurrencyPerformance::test_thread_safety_image_operations PASSED [ 32%]\ntests/performance/test_performance.py::TestResourceUtilization::test_memory_usage_patterns PASSED [ 32%]\ntests/performance/test_performance.py::TestResourceUtilization::test_cpu_utilization_efficiency PASSED [ 32%]\ntests/performance/test_performance.py::TestResourceUtilization::test_file_handle_management PASSED [ 32%]\ntests/performance/test_performance.py::TestScalabilityLimits::test_maximum_image_dimensions PASSED [ 32%]\ntests/performance/test_performance.py::TestScalabilityLimits::test_maximum_tile_count PASSED [ 33%]\ntests/performance/test_performance.py::TestScalabilityLimits::test_stress_concurrent_operations PASSED [ 33%]\ntests/unit/test_core/test_config.py::TestConfig::test_default_config PASSED [ 33%]\ntests/unit/test_core/test_config.py::TestConfig::test_config_with_custom_values PASSED [ 33%]\ntests/unit/test_core/test_config.py::TestConfig::test_load_from_file PASSED [ 34%]\ntests/unit/test_core/test_config.py::TestConfig::test_load_from_nonexistent_file PASSED [ 34%]\ntests/unit/test_core/test_config.py::TestConfig::test_save_to_file PASSED [ 34%]\ntests/unit/test_core/test_config.py::TestConfig::test_load_from_env PASSED [ 34%]\ntests/unit/test_core/test_config.py::TestConfig::test_get_tool_config PASSED [ 34%]\ntests/unit/test_core/test_config.py::TestConfig::test_set_tool_config PASSED [ 35%]\ntests/unit/test_core/test_config.py::TestLoggingConfig::test_default_logging_config PASSED [ 35%]\ntests/unit/test_core/test_config.py::TestLoggingConfig::test_custom_logging_config PASSED [ 35%]\ntests/unit/test_core/test_config.py::TestPerformanceConfig::test_default_performance_config PASSED [ 35%]\ntests/unit/test_core/test_config.py::TestPerformanceConfig::test_custom_performance_config PASSED [ 35%]\ntests/unit/test_core/test_config.py::TestOutputConfig::test_default_output_config PASSED [ 36%]\ntests/unit/test_core/test_config.py::TestOutputConfig::test_custom_output_config PASSED [ 36%]\ntests/unit/test_core/test_config.py::TestOutputConfig::test_invalid_quality_value PASSED [ 36%]\ntests/unit/test_core/test_exceptions.py::TestErrorCode::test_error_code_values PASSED [ 36%]\ntests/unit/test_core/test_exceptions.py::TestErrorCode::test_error_code_string_comparison FAILED [ 36%]\ntests/unit/test_core/test_exceptions.py::TestErrorCode::test_error_code_categories PASSED [ 37%]\ntests/unit/test_core/test_exceptions.py::TestRetileupError::test_retileup_error_minimal PASSED [ 37%]\ntests/unit/test_core/test_exceptions.py::TestRetileupError::test_retileup_error_complete PASSED [ 37%]\ntests/unit/test_core/test_exceptions.py::TestRetileupError::test_retileup_error_to_dict PASSED [ 37%]\ntests/unit/test_core/test_exceptions.py::TestRetileupError::test_retileup_error_to_dict_no_cause PASSED [ 37%]\ntests/unit/test_core/test_exceptions.py::TestRetileupError::test_retileup_error_string_representation PASSED [ 38%]\ntests/unit/test_core/test_exceptions.py::TestRetileupError::test_retileup_error_repr PASSED [ 38%]\ntests/unit/test_core/test_exceptions.py::TestRetileupError::test_retileup_error_inheritance PASSED [ 38%]\ntests/unit/test_core/test_exceptions.py::TestValidationError::test_validation_error_minimal PASSED [ 38%]\ntests/unit/test_core/test_exceptions.py::TestValidationError::test_validation_error_with_field_info PASSED [ 38%]\ntests/unit/test_core/test_exceptions.py::TestValidationError::test_validation_error_custom_error_code PASSED [ 39%]\ntests/unit/test_core/test_exceptions.py::TestProcessingError::test_processing_error_minimal PASSED [ 39%]\ntests/unit/test_core/test_exceptions.py::TestProcessingError::test_processing_error_with_tool_info PASSED [ 39%]\ntests/unit/test_core/test_exceptions.py::TestProcessingError::test_processing_error_custom_error_code PASSED [ 39%]\ntests/unit/test_core/test_exceptions.py::TestConfigurationError::test_configuration_error_minimal PASSED [ 39%]\ntests/unit/test_core/test_exceptions.py::TestConfigurationError::test_configuration_error_with_path_info PASSED [ 40%]\ntests/unit/test_core/test_exceptions.py::TestWorkflowError::test_workflow_error_minimal PASSED [ 40%]\ntests/unit/test_core/test_exceptions.py::TestWorkflowError::test_workflow_error_with_workflow_info PASSED [ 40%]\ntests/unit/test_core/test_exceptions.py::TestRegistryError::test_registry_error_minimal PASSED [ 40%]\ntests/unit/test_core/test_exceptions.py::TestRegistryError::test_registry_error_with_tool_info PASSED [ 40%]\ntests/unit/test_core/test_exceptions.py::TestSecurityError::test_security_error_minimal PASSED [ 41%]\ntests/unit/test_core/test_exceptions.py::TestSecurityError::test_security_error_with_security_info PASSED [ 41%]\ntests/unit/test_core/test_exceptions.py::TestResourceError::test_resource_error_minimal PASSED [ 41%]\ntests/unit/test_core/test_exceptions.py::TestResourceError::test_resource_error_with_resource_info PASSED [ 41%]\ntests/unit/test_core/test_exceptions.py::TestConvenienceFunctions::test_validation_error_function PASSED [ 41%]\ntests/unit/test_core/test_exceptions.py::TestConvenienceFunctions::test_validation_error_function_with_cause PASSED [ 42%]\ntests/unit/test_core/test_exceptions.py::TestConvenienceFunctions::test_processing_error_function PASSED [ 42%]\ntests/unit/test_core/test_exceptions.py::TestConvenienceFunctions::test_configuration_error_function PASSED [ 42%]\ntests/unit/test_core/test_exceptions.py::TestConvenienceFunctions::test_workflow_error_function PASSED [ 42%]\ntests/unit/test_core/test_exceptions.py::TestConvenienceFunctions::test_registry_error_function PASSED [ 42%]\ntests/unit/test_core/test_exceptions.py::TestErrorChaining::test_simple_error_chain PASSED [ 43%]\ntests/unit/test_core/test_exceptions.py::TestErrorChaining::test_nested_error_chain FAILED [ 43%]\ntests/unit/test_core/test_exceptions.py::TestErrorChaining::test_error_serialization_with_cause FAILED [ 43%]\ntests/unit/test_core/test_exceptions.py::TestErrorContextHandling::test_context_merging PASSED [ 43%]\ntests/unit/test_core/test_exceptions.py::TestErrorContextHandling::test_context_override_protection PASSED [ 43%]\ntests/unit/test_core/test_exceptions.py::TestErrorContextHandling::test_complex_context_types PASSED [ 44%]\ntests/unit/test_core/test_exceptions.py::TestErrorUsagePatterns::test_exception_handling_by_type PASSED [ 44%]\ntests/unit/test_core/test_exceptions.py::TestErrorUsagePatterns::test_exception_handling_by_base_class PASSED [ 44%]\ntests/unit/test_core/test_exceptions.py::TestErrorUsagePatterns::test_error_code_based_handling PASSED [ 44%]\ntests/unit/test_core/test_exceptions.py::TestErrorUsagePatterns::test_error_logging_format PASSED [ 45%]\ntests/unit/test_core/test_registry.py::TestToolMetadata::test_tool_metadata_creation PASSED [ 45%]\ntests/unit/test_core/test_registry.py::TestToolMetadata::test_tool_metadata_to_dict PASSED [ 45%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_registry_initialization PASSED [ 45%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_tool_registration_success PASSED [ 45%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_tool_registration_with_custom_name PASSED [ 46%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_tool_registration_duplicate_without_force PASSED [ 46%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_tool_registration_duplicate_with_force PASSED [ 46%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_tool_registration_invalid_class PASSED [ 46%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_tool_registration_abstract_base_class PASSED [ 46%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_tool_registration_failing_tool FAILED [ 47%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_tool_unregistration PASSED [ 47%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_get_tool_class_with_usage_tracking PASSED [ 47%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_get_tool_class_nonexistent PASSED [ 47%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_create_tool_instance PASSED [ 47%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_create_tool_instance_nonexistent PASSED [ 48%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_create_tool_instance_failure FAILED [ 48%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_get_tool_metadata PASSED [ 48%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_list_tools PASSED [ 48%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_list_tools_by_pattern PASSED [ 48%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_get_tool_statistics PASSED [ 49%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_clear_registry PASSED [ 49%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_export_registry_state PASSED [ 49%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_registry_len_contains_iter PASSED [ 49%]\ntests/unit/test_core/test_registry.py::TestToolRegistry::test_registry_string_representation PASSED [ 49%]\ntests/unit/test_core/test_registry.py::TestToolRegistryPluginLoading::test_add_plugin_directory PASSED [ 50%]\ntests/unit/test_core/test_registry.py::TestToolRegistryPluginLoading::test_add_nonexistent_plugin_directory PASSED [ 50%]\ntests/unit/test_core/test_registry.py::TestToolRegistryPluginLoading::test_load_plugins_from_directory PASSED [ 50%]\ntests/unit/test_core/test_registry.py::TestToolRegistryPluginLoading::test_load_plugins_from_nonexistent_directory PASSED [ 50%]\ntests/unit/test_core/test_registry.py::TestToolRegistryPluginLoading::test_load_plugins_skip_private_files PASSED [ 50%]\ntests/unit/test_core/test_registry.py::TestToolRegistryPluginLoading::test_auto_discovery PASSED [ 51%]\ntests/unit/test_core/test_registry.py::TestToolRegistryPluginLoading::test_auto_discovery_disabled PASSED [ 51%]\ntests/unit/test_core/test_registry.py::TestToolRegistryPluginLoading::test_auto_discovery_caching PASSED [ 51%]\ntests/unit/test_core/test_registry.py::TestToolRegistryHealthAndValidation::test_validate_tool_health_success PASSED [ 51%]\ntests/unit/test_core/test_registry.py::TestToolRegistryHealthAndValidation::test_validate_tool_health_nonexistent PASSED [ 51%]\ntests/unit/test_core/test_registry.py::TestToolRegistryHealthAndValidation::test_validate_tool_health_failure FAILED [ 52%]\ntests/unit/test_core/test_registry.py::TestToolRegistryThreadSafety::test_concurrent_registration PASSED [ 52%]\ntests/unit/test_core/test_registry.py::TestToolRegistryThreadSafety::test_concurrent_access FAILED [ 52%]\ntests/unit/test_core/test_registry.py::TestGlobalRegistry::test_get_global_registry PASSED [ 52%]\ntests/unit/test_core/test_registry.py::TestGlobalRegistry::test_reset_global_registry PASSED [ 52%]\ntests/unit/test_core/test_registry.py::TestGlobalRegistry::test_global_registry_thread_safety PASSED [ 53%]\ntests/unit/test_tools/test_base.py::TestToolResult::test_tool_result_creation_minimal PASSED [ 53%]\ntests/unit/test_tools/test_base.py::TestToolResult::test_tool_result_creation_complete PASSED [ 53%]\ntests/unit/test_tools/test_base.py::TestToolResult::test_tool_result_path_validation PASSED [ 53%]\ntests/unit/test_tools/test_base.py::TestToolResult::test_tool_result_negative_execution_time PASSED [ 53%]\ntests/unit/test_tools/test_base.py::TestToolResult::test_tool_result_immutable_after_creation PASSED [ 54%]\ntests/unit/test_tools/test_base.py::TestToolResult::test_tool_result_extra_fields_forbidden PASSED [ 54%]\ntests/unit/test_tools/test_base.py::TestToolConfig::test_tool_config_creation_minimal PASSED [ 54%]\ntests/unit/test_tools/test_base.py::TestToolConfig::test_tool_config_creation_complete PASSED [ 54%]\ntests/unit/test_tools/test_base.py::TestToolConfig::test_tool_config_path_validation PASSED [ 54%]\ntests/unit/test_tools/test_base.py::TestToolConfig::test_tool_config_invalid_timeout PASSED [ 55%]\ntests/unit/test_tools/test_base.py::TestToolConfig::test_tool_config_extra_fields_forbidden PASSED [ 55%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_base_tool_cannot_be_instantiated PASSED [ 55%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_concrete_tool_instantiation PASSED [ 55%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_basic_execution PASSED [ 56%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_execution_with_timing PASSED [ 56%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_lifecycle_management PASSED [ 56%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_manual_lifecycle PASSED [ 56%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_config_validation_success PASSED [ 56%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_config_validation_failure PASSED [ 57%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_execution_failure PASSED [ 57%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_execution_failure_with_timing PASSED [ 57%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_string_representations PASSED [ 57%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_get_config_schema PASSED [ 57%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_execution_timing_precision PASSED [ 58%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_config_combinations[True-False] PASSED [ 58%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_config_combinations[False-True] PASSED [ 58%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_config_combinations[True-True] PASSED [ 58%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_config_combinations[False-False] PASSED [ 58%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_metadata_preservation PASSED [ 59%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_exception_propagation PASSED [ 59%]\ntests/unit/test_tools/test_base.py::TestBaseTool::test_tool_concurrent_execution PASSED [ 59%]\ntests/unit/test_tools/test_base.py::TestToolEdgeCases::test_tool_with_empty_name PASSED [ 59%]\ntests/unit/test_tools/test_base.py::TestToolEdgeCases::test_tool_with_unicode_properties PASSED [ 59%]\ntests/unit/test_tools/test_base.py::TestToolEdgeCases::test_tool_with_very_long_properties PASSED [ 60%]\ntests/unit/test_tools/test_base.py::TestToolEdgeCases::test_tool_setup_cleanup_idempotency PASSED [ 60%]\ntests/unit/test_tools/test_base.py::TestToolEdgeCases::test_tool_execution_with_none_config FAILED [ 60%]\ntests/unit/test_tools/test_base.py::TestToolEdgeCases::test_tool_memory_efficiency FAILED [ 60%]\ntests/unit/test_tools/test_tiling.py::TestTilingConfig::test_valid_config PASSED [ 60%]\ntests/unit/test_tools/test_tiling.py::TestTilingConfig::test_invalid_tile_dimensions FAILED [ 61%]\ntests/unit/test_tools/test_tiling.py::TestTilingConfig::test_invalid_coordinates FAILED [ 61%]\ntests/unit/test_tools/test_tiling.py::TestTilingConfig::test_invalid_output_pattern PASSED [ 61%]\ntests/unit/test_tools/test_tiling.py::TestTilingConfig::test_overlap_validation PASSED [ 61%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_tool_properties PASSED [ 61%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_config_validation_success PASSED [ 62%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_config_validation_missing_file PASSED [ 62%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_config_validation_coordinates_out_of_bounds PASSED [ 62%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_successful_execution PASSED [ 62%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_dry_run_execution PASSED [ 62%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_overlap_functionality PASSED [ 63%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_maintain_aspect_ratio PASSED [ 63%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_custom_output_pattern PASSED [ 63%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_error_handling_corrupted_image PASSED [ 63%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_memory_estimation PASSED [ 63%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_invalid_config_type PASSED [ 64%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_partial_failure_recovery PASSED [ 64%]\ntests/unit/test_tools/test_tiling.py::TestTilingTool::test_tool_lifecycle PASSED [ 64%]\ntests/unit/test_tools/test_tiling.py::TestTilingToolPerformance::test_large_image_processing PASSED [ 64%]\ntests/unit/test_tools/test_tiling.py::TestTilingToolPerformance::test_memory_efficiency PASSED [ 64%]\ntests/unit/test_tools/test_tiling.py::TestTilingToolFormats::test_png_format PASSED [ 65%]\ntests/unit/test_tools/test_tiling.py::TestTilingToolFormats::test_jpeg_format PASSED [ 65%]\ntests/unit/test_tools/test_tiling.py::TestTilingToolFormats::test_rgba_to_rgb_conversion FAILED [ 65%]\ntests/unit/test_utils/test_image.py::TestImageLoading::test_load_image_success ERROR [ 65%]\ntests/unit/test_utils/test_image.py::TestImageLoading::test_load_image_with_conversion ERROR [ 65%]\ntests/unit/test_utils/test_image.py::TestImageLoading::test_load_image_file_not_found PASSED [ 66%]\ntests/unit/test_utils/test_image.py::TestImageLoading::test_load_image_not_a_file PASSED [ 66%]\ntests/unit/test_utils/test_image.py::TestImageLoading::test_load_image_empty_file PASSED [ 66%]\ntests/unit/test_utils/test_image.py::TestImageLoading::test_load_image_invalid_format PASSED [ 66%]\ntests/unit/test_utils/test_image.py::TestImageLoading::test_load_image_palette_mode PASSED [ 67%]\ntests/unit/test_utils/test_image.py::TestImageLoading::test_load_image_cmyk_mode PASSED [ 67%]\ntests/unit/test_utils/test_image.py::TestImageLoading::test_load_image_with_exif ERROR [ 67%]\ntests/unit/test_utils/test_image.py::TestImageModeConversion::test_convert_rgba_to_rgb PASSED [ 67%]\ntests/unit/test_utils/test_image.py::TestImageModeConversion::test_convert_rgb_to_grayscale PASSED [ 67%]\ntests/unit/test_utils/test_image.py::TestImageModeConversion::test_convert_same_mode PASSED [ 68%]\ntests/unit/test_utils/test_image.py::TestImageModeConversion::test_convert_standard_conversion PASSED [ 68%]\ntests/unit/test_utils/test_image.py::TestImageSaving::test_save_image_basic PASSED [ 68%]\ntests/unit/test_utils/test_image.py::TestImageSaving::test_save_image_with_format PASSED [ 68%]\ntests/unit/test_utils/test_image.py::TestImageSaving::test_save_image_creates_directory PASSED [ 68%]\ntests/unit/test_utils/test_image.py::TestImageSaving::test_save_image_infers_format_from_extension PASSED [ 69%]\ntests/unit/test_utils/test_image.py::TestImageSaving::test_save_image_error_handling PASSED [ 69%]\ntests/unit/test_utils/test_image.py::TestImageInfo::test_get_image_info_basic PASSED [ 69%]\ntests/unit/test_utils/test_image.py::TestImageInfo::test_get_image_info_with_transparency PASSED [ 69%]\ntests/unit/test_utils/test_image.py::TestImageInfo::test_get_image_info_with_file_size ERROR [ 69%]\ntests/unit/test_utils/test_image.py::TestImageInfo::test_get_image_info_with_exif ERROR [ 70%]\ntests/unit/test_utils/test_image.py::TestImageManipulation::test_resize_image_maintain_aspect PASSED [ 70%]\ntests/unit/test_utils/test_image.py::TestImageManipulation::test_resize_image_no_aspect PASSED [ 70%]\ntests/unit/test_utils/test_image.py::TestImageManipulation::test_resize_image_different_methods PASSED [ 70%]\ntests/unit/test_utils/test_image.py::TestImageManipulation::test_resize_image_invalid_method PASSED [ 70%]\ntests/unit/test_utils/test_image.py::TestImageManipulation::test_crop_image PASSED [ 71%]\ntests/unit/test_utils/test_image.py::TestImageManipulation::test_rotate_image PASSED [ 71%]\ntests/unit/test_utils/test_image.py::TestImageManipulation::test_rotate_image_no_expand PASSED [ 71%]\ntests/unit/test_utils/test_image.py::TestImageManipulation::test_flip_image_horizontal PASSED [ 71%]\ntests/unit/test_utils/test_image.py::TestImageManipulation::test_flip_image_vertical PASSED [ 71%]\ntests/unit/test_utils/test_image.py::TestImageManipulation::test_auto_orient PASSED [ 72%]\ntests/unit/test_utils/test_image.py::TestImageValidation::test_get_supported_formats PASSED [ 72%]\ntests/unit/test_utils/test_image.py::TestImageValidation::test_is_valid_image_success ERROR [ 72%]\ntests/unit/test_utils/test_image.py::TestImageValidation::test_is_valid_image_failure PASSED [ 72%]\ntests/unit/test_utils/test_image.py::TestImageValidation::test_is_valid_image_nonexistent PASSED [ 72%]\ntests/unit/test_utils/test_image.py::TestImageValidation::test_validate_image_format_success ERROR [ 73%]\ntests/unit/test_utils/test_image.py::TestImageValidation::test_validate_image_format_file_not_found PASSED [ 73%]\ntests/unit/test_utils/test_image.py::TestImageValidation::test_validate_image_format_invalid_image PASSED [ 73%]\ntests/unit/test_utils/test_image.py::TestImageCalculations::test_calculate_file_size_rgb PASSED [ 73%]\ntests/unit/test_utils/test_image.py::TestImageCalculations::test_calculate_file_size_rgba PASSED [ 73%]\ntests/unit/test_utils/test_image.py::TestImageCalculations::test_calculate_file_size_grayscale PASSED [ 74%]\ntests/unit/test_utils/test_image.py::TestImageCalculations::test_calculate_file_size_with_compression PASSED [ 74%]\ntests/unit/test_utils/test_image.py::TestImageCalculations::test_calculate_file_size_unknown_mode PASSED [ 74%]\ntests/unit/test_utils/test_image.py::TestImageCalculations::test_get_safe_crop_bounds_normal PASSED [ 74%]\ntests/unit/test_utils/test_image.py::TestImageCalculations::test_get_safe_crop_bounds_with_overlap PASSED [ 74%]\ntests/unit/test_utils/test_image.py::TestImageCalculations::test_get_safe_crop_bounds_exceeds_image PASSED [ 75%]\ntests/unit/test_utils/test_image.py::TestImageCalculations::test_get_safe_crop_bounds_negative_start PASSED [ 75%]\ntests/unit/test_utils/test_image.py::TestImageCalculations::test_estimate_processing_memory PASSED [ 75%]\ntests/unit/test_utils/test_image.py::TestImageCalculations::test_estimate_processing_memory_zero_tiles PASSED [ 75%]\ntests/unit/test_utils/test_image.py::TestImageOptimization::test_optimize_image_for_tiling PASSED [ 75%]\ntests/unit/test_utils/test_image.py::TestImageOptimization::test_optimize_image_converts_unusual_mode PASSED [ 76%]\ntests/unit/test_utils/test_image.py::TestTileInfo::test_create_tile_info_success ERROR [ 76%]\ntests/unit/test_utils/test_image.py::TestTileInfo::test_create_tile_info_with_overlap ERROR [ 76%]\ntests/unit/test_utils/test_image.py::TestTileInfo::test_create_tile_info_invalid_tiles ERROR [ 76%]\ntests/unit/test_utils/test_image.py::TestTileInfo::test_create_tile_info_memory_feasibility ERROR [ 76%]\ntests/unit/test_utils/test_image.py::TestTileInfo::test_create_tile_info_error_handling PASSED [ 77%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_load_image_with_warning_mime_type PASSED [ 77%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_save_image_with_additional_kwargs PASSED [ 77%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_color_mode_edge_cases PASSED [ 77%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_large_image_memory_estimation PASSED [ 78%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_zero_dimension_handling PASSED [ 78%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_thread_safety_image_operations ERROR [ 78%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_memory_efficiency_large_operations ERROR [ 78%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_format_normalization_edge_cases PASSED [ 78%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_image_info_robustness PASSED [ 79%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_calculate_file_size_modes[L-1] PASSED [ 79%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_calculate_file_size_modes[RGB-3] PASSED [ 79%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_calculate_file_size_modes[RGBA-4] PASSED [ 79%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_calculate_file_size_modes[CMYK-4] PASSED [ 79%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_calculate_file_size_modes[P-1] PASSED [ 80%]\ntests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_calculate_file_size_modes[UNKNOWN-3] PASSED [ 80%]\ntests/unit/test_utils/test_validation.py::TestValidationResult::test_validation_result_success PASSED [ 80%]\ntests/unit/test_utils/test_validation.py::TestValidationResult::test_validation_result_failure PASSED [ 80%]\ntests/unit/test_utils/test_validation.py::TestValidationResult::test_add_error PASSED [ 80%]\ntests/unit/test_utils/test_validation.py::TestValidationResult::test_merge_validation_results PASSED [ 81%]\ntests/unit/test_utils/test_validation.py::TestValidationResult::test_merge_successful_results PASSED [ 81%]\ntests/unit/test_utils/test_validation.py::TestValidationDecorators::test_validate_input_success PASSED [ 81%]\ntests/unit/test_utils/test_validation.py::TestValidationDecorators::test_validate_input_failure PASSED [ 81%]\ntests/unit/test_utils/test_validation.py::TestValidationDecorators::test_validate_input_validator_exception PASSED [ 81%]\ntests/unit/test_utils/test_validation.py::TestValidationDecorators::test_validate_input_with_kwargs PASSED [ 82%]\ntests/unit/test_utils/test_validation.py::TestValidationDecorators::test_validate_input_missing_parameter PASSED [ 82%]\ntests/unit/test_utils/test_validation.py::TestConfigValidation::test_validate_config_success PASSED [ 82%]\ntests/unit/test_utils/test_validation.py::TestConfigValidation::test_validate_config_dict_conversion PASSED [ 82%]\ntests/unit/test_utils/test_validation.py::TestConfigValidation::test_validate_config_invalid_dict PASSED [ 82%]\ntests/unit/test_utils/test_validation.py::TestConfigValidation::test_validate_config_wrong_type PASSED [ 83%]\ntests/unit/test_utils/test_validation.py::TestConfigValidation::test_validate_config_no_args PASSED [ 83%]\ntests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_file_path_factory_existing_file ERROR [ 83%]\ntests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_file_path_factory_nonexistent PASSED [ 83%]\ntests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_file_path_factory_allow_nonexistent PASSED [ 83%]\ntests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_file_path_factory_directory PASSED [ 84%]\ntests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_file_path_factory_permissions FAILED [ 84%]\ntests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_file_path_factory_error_handling PASSED [ 84%]\ntests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_image_file_factory_success ERROR [ 84%]\ntests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_image_file_factory_unsupported_format PASSED [ 84%]\ntests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_image_file_factory_size_limit PASSED [ 85%]\ntests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_image_file_factory_error_handling FAILED [ 85%]\ntests/unit/test_utils/test_validation.py::TestNumericValidation::test_validate_positive_number_success PASSED [ 85%]\ntests/unit/test_utils/test_validation.py::TestNumericValidation::test_validate_positive_number_failure PASSED [ 85%]\ntests/unit/test_utils/test_validation.py::TestNumericValidation::test_validate_non_negative_number_success PASSED [ 85%]\ntests/unit/test_utils/test_validation.py::TestNumericValidation::test_validate_non_negative_number_failure PASSED [ 86%]\ntests/unit/test_utils/test_validation.py::TestNumericValidation::test_validate_in_range_inclusive PASSED [ 86%]\ntests/unit/test_utils/test_validation.py::TestNumericValidation::test_validate_in_range_exclusive PASSED [ 86%]\ntests/unit/test_utils/test_validation.py::TestNumericValidation::test_validate_in_range_invalid_type PASSED [ 86%]\ntests/unit/test_utils/test_validation.py::TestNumericValidation::test_validate_coordinates_success PASSED [ 86%]\ntests/unit/test_utils/test_validation.py::TestNumericValidation::test_validate_coordinates_failure PASSED [ 87%]\ntests/unit/test_utils/test_validation.py::TestValidationContext::test_validation_context_success PASSED [ 87%]\ntests/unit/test_utils/test_validation.py::TestValidationContext::test_validation_context_add_error PASSED [ 87%]\ntests/unit/test_utils/test_validation.py::TestValidationContext::test_validation_context_add_warning PASSED [ 87%]\ntests/unit/test_utils/test_validation.py::TestValidationContext::test_validation_context_error_summary PASSED [ 87%]\ntests/unit/test_utils/test_validation.py::TestValidationContext::test_validation_context_raise_if_errors PASSED [ 88%]\ntests/unit/test_utils/test_validation.py::TestValidationContext::test_validation_context_no_raise_if_no_errors PASSED [ 88%]\ntests/unit/test_utils/test_validation.py::TestValidationContext::test_validation_context_manager_success PASSED [ 88%]\ntests/unit/test_utils/test_validation.py::TestValidationContext::test_validation_context_manager_with_errors PASSED [ 88%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_file_path_success ERROR [ 89%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_file_path_not_exists PASSED [ 89%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_file_path_not_file PASSED [ 89%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_file_path_extension PASSED [ 89%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_directory_path_success PASSED [ 89%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_directory_path_create_missing PASSED [ 90%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_directory_path_creation_error FAILED [ 90%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_image_size_success PASSED [ 90%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_image_size_failure PASSED [ 90%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_numeric_range_success PASSED [ 90%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_numeric_range_failure PASSED [ 91%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_percentage_success PASSED [ 91%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_percentage_failure PASSED [ 91%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_string_pattern_success PASSED [ 91%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_string_pattern_failure PASSED [ 91%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_string_pattern_invalid_regex PASSED [ 92%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_choice_success PASSED [ 92%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_choice_failure PASSED [ 92%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_pydantic_model_success PASSED [ 92%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_pydantic_model_failure PASSED [ 92%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_workflow_parameters_success PASSED [ 93%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_workflow_parameters_missing_required PASSED [ 93%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_workflow_parameters_unknown PASSED [ 93%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_image_format_success PASSED [ 93%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_image_format_case_insensitive PASSED [ 93%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_image_format_failure PASSED [ 94%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_color_value_hex_success PASSED [ 94%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_color_value_hex_with_alpha PASSED [ 94%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_color_value_hex_failure PASSED [ 94%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_color_value_rgb_success PASSED [ 94%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_color_value_rgba_success PASSED [ 95%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_color_value_rgb_failure PASSED [ 95%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_color_value_grayscale_success PASSED [ 95%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_color_value_grayscale_failure PASSED [ 95%]\ntests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_color_value_invalid_type PASSED [ 95%]\ntests/unit/test_utils/test_validation.py::TestCommonValidators::test_positive_int_validator PASSED [ 96%]\ntests/unit/test_utils/test_validation.py::TestCommonValidators::test_non_negative_int_validator PASSED [ 96%]\ntests/unit/test_utils/test_validation.py::TestCommonValidators::test_positive_float_validator PASSED [ 96%]\ntests/unit/test_utils/test_validation.py::TestCommonValidators::test_non_empty_string_validator PASSED [ 96%]\ntests/unit/test_utils/test_validation.py::TestCommonValidators::test_existing_dir_validator PASSED [ 96%]\ntests/unit/test_utils/test_validation.py::TestCommonValidators::test_coordinates_validator PASSED [ 97%]\ntests/unit/test_utils/test_validation.py::TestBatchValidation::test_batch_validate_success PASSED [ 97%]\ntests/unit/test_utils/test_validation.py::TestBatchValidation::test_batch_validate_with_errors PASSED [ 97%]\ntests/unit/test_utils/test_validation.py::TestBatchValidation::test_batch_validate_missing_field PASSED [ 97%]\ntests/unit/test_utils/test_validation.py::TestBatchValidation::test_batch_validate_raise_on_error PASSED [ 97%]\ntests/unit/test_utils/test_validation.py::TestBatchValidation::test_batch_validate_validator_exception PASSED [ 98%]\ntests/unit/test_utils/test_validation.py::TestValidationEdgeCases::test_path_validation_with_string_input ERROR [ 98%]\ntests/unit/test_utils/test_validation.py::TestValidationEdgeCases::test_validation_with_none_values PASSED [ 98%]\ntests/unit/test_utils/test_validation.py::TestValidationEdgeCases::test_image_validation_without_pil PASSED [ 98%]\ntests/unit/test_utils/test_validation.py::TestValidationEdgeCases::test_validation_context_unicode_messages PASSED [ 98%]\ntests/unit/test_utils/test_validation.py::TestValidationEdgeCases::test_range_validator_edge_values PASSED [ 99%]\ntests/unit/test_utils/test_validation.py::TestValidationEdgeCases::test_coordinate_validation_edge_cases PASSED [ 99%]\ntests/unit/test_utils/test_validation.py::TestValidationEdgeCases::test_validation_decorator_with_complex_signature PASSED [ 99%]\ntests/unit/test_utils/test_validation.py::TestValidationEdgeCases::test_memory_efficiency_large_validation PASSED [ 99%]\ntests/unit/test_utils/test_validation.py::TestValidationEdgeCases::test_validation_thread_safety PASSED [100%]\nERROR: Coverage failure: total of 49.22 is less than fail-under=90.00\n\n\n==================================== ERRORS ====================================\n_____ ERROR at setup of TestWorkflowBasics.test_workflow_tool_registration _____\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n______ ERROR at setup of TestWorkflowBasics.test_workflow_tool_retrieval _______\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n____ ERROR at setup of TestWorkflowExecution.test_simple_workflow_execution ____\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n___ ERROR at setup of TestWorkflowExecution.test_workflow_with_output_files ____\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n___ ERROR at setup of TestWorkflowExecution.test_workflow_execution_failure ____\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n_ ERROR at setup of TestWorkflowExecution.test_workflow_execution_with_timing __\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n__ ERROR at setup of TestWorkflowExecution.test_workflow_multiple_executions ___\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n_ ERROR at setup of TestWorkflowExecution.test_workflow_parallel_configuration _\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n_ ERROR at setup of TestWorkflowConcurrency.test_concurrent_workflow_execution _\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n____ ERROR at setup of TestWorkflowConcurrency.test_workflow_thread_safety _____\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n_ ERROR at setup of TestWorkflowConcurrency.test_workflow_resource_contention __\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n_ ERROR at setup of TestWorkflowErrorHandling.test_workflow_validation_error_handling _\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n_ ERROR at setup of TestWorkflowErrorHandling.test_workflow_processing_error_recovery _\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n__ ERROR at setup of TestWorkflowErrorHandling.test_workflow_timeout_handling __\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n_ ERROR at setup of TestWorkflowErrorHandling.test_workflow_partial_failure_handling _\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n_ ERROR at setup of TestWorkflowErrorHandling.test_workflow_cleanup_on_failure _\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n____ ERROR at setup of TestWorkflowPerformance.test_workflow_execution_time ____\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n__ ERROR at setup of TestWorkflowPerformance.test_workflow_memory_efficiency ___\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n_____ ERROR at setup of TestWorkflowPerformance.test_workflow_scalability ______\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n___ ERROR at setup of TestWorkflowPerformance.test_workflow_resource_cleanup ___\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n______ ERROR at setup of TestWorkflowIntegration.test_end_to_end_workflow ______\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n_______ ERROR at setup of TestWorkflowIntegration.test_workflow_chaining _______\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n__ ERROR at setup of TestWorkflowIntegration.test_workflow_error_propagation ___\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n_ ERROR at setup of TestWorkflowIntegration.test_workflow_configuration_inheritance _\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n___ ERROR at setup of TestWorkflowIntegration.test_workflow_state_management ___\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n>       if not issubclass(tool_class, BaseTool):\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       TypeError: issubclass() arg 1 must be a class\n\nsrc/retileup/core/registry.py:204: TypeError\n\nDuring handling of the above exception, another exception occurred:\n\n    @pytest.fixture\n    def mock_registry():\n        \"\"\"Create a mock tool registry with test tools.\"\"\"\n        registry = ToolRegistry()\n>       registry.register_tool(MockWorkflowTool())\n\ntests/integration/test_workflows.py:97: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <MockWorkflowTool(name='mock-workflow-tool', version='1.0.0')>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n                raise registry_error(\n>                   f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                                               ^^^^^^^^^^^^^^^^^^^\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               AttributeError: 'MockWorkflowTool' object has no attribute '__name__'\n\nsrc/retileup/core/registry.py:188: AttributeError\n__________ ERROR at setup of TestImageLoading.test_load_image_success __________\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 27\n      def test_load_image_success(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:27\n______ ERROR at setup of TestImageLoading.test_load_image_with_conversion ______\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 35\n      def test_load_image_with_conversion(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:35\n_________ ERROR at setup of TestImageLoading.test_load_image_with_exif _________\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 96\n      def test_load_image_with_exif(self, sample_image_with_exif):\nE       fixture 'sample_image_with_exif' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:96\n______ ERROR at setup of TestImageInfo.test_get_image_info_with_file_size ______\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 222\n      def test_get_image_info_with_file_size(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:222\n________ ERROR at setup of TestImageInfo.test_get_image_info_with_exif _________\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 232\n      def test_get_image_info_with_exif(self, sample_image_with_exif):\nE       fixture 'sample_image_with_exif' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:232\n______ ERROR at setup of TestImageValidation.test_is_valid_image_success _______\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 358\n      def test_is_valid_image_success(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:358\n___ ERROR at setup of TestImageValidation.test_validate_image_format_success ___\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 373\n      def test_validate_image_format_success(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:373\n_________ ERROR at setup of TestTileInfo.test_create_tile_info_success _________\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 512\n      def test_create_tile_info_success(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:512\n______ ERROR at setup of TestTileInfo.test_create_tile_info_with_overlap _______\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 531\n      def test_create_tile_info_with_overlap(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:531\n______ ERROR at setup of TestTileInfo.test_create_tile_info_invalid_tiles ______\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 548\n      def test_create_tile_info_invalid_tiles(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:548\n___ ERROR at setup of TestTileInfo.test_create_tile_info_memory_feasibility ____\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 561\n      def test_create_tile_info_memory_feasibility(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:561\n_ ERROR at setup of TestImageUtilsEdgeCases.test_thread_safety_image_operations _\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 649\n      def test_thread_safety_image_operations(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:649\n_ ERROR at setup of TestImageUtilsEdgeCases.test_memory_efficiency_large_operations _\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py, line 687\n      def test_memory_efficiency_large_operations(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_image.py:687\n_ ERROR at setup of TestFileValidation.test_validate_file_path_factory_existing_file _\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_validation.py, line 215\n      def test_validate_file_path_factory_existing_file(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_validation.py:215\n_ ERROR at setup of TestFileValidation.test_validate_image_file_factory_success _\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_validation.py, line 261\n      def test_validate_image_file_factory_success(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_validation.py:261\n____ ERROR at setup of TestValidationUtils.test_validate_file_path_success _____\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_validation.py, line 455\n      def test_validate_file_path_success(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_validation.py:455\n_ ERROR at setup of TestValidationEdgeCases.test_path_validation_with_string_input _\nfile /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_validation.py, line 836\n      def test_path_validation_with_string_input(self, sample_image_rgb):\nE       fixture 'sample_image_rgb' not found\n>       available fixtures: _class_scoped_runner, _function_scoped_runner, _module_scoped_runner, _package_scoped_runner, _session_scoped_runner, benchmark_images, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, clean_registry, cli_runner, complex_config, complex_image, config, config_file, corrupted_image_file, cov, doctest_namespace, empty_directory, error_scenarios, event_loop_policy, failing_mock_tool, huge_image_files, invalid_coordinates, invalid_image_file, isolated_filesystem, large_image, memory_monitor, minimal_config, mixed_format_images, mock_database, mock_network_conditions, mock_storage, mock_tool, mocker, module_mocker, monkeypatch, multiple_image_files, no_cover, package_mocker, performance_timer, platform_paths, populated_registry, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_coordinates, sample_grayscale_image, sample_image, sample_image_file, sample_image_with_alpha, sample_jpeg_file, sample_rgba_image, sample_step_data, sample_tiling_config, sample_tool_metadata, sample_workflow, sample_workflow_data, session_mocker, setup_test_environment, temp_dir, temp_output_dir, testrun_uid, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tool_registry, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory, very_large_image, worker_id, workflow_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/unit/test_utils/test_validation.py:836\n=================================== FAILURES ===================================\n_________ TestCorruptedDataHandling.test_malformed_configuration_data __________\n\nself = <test_edge_cases.TestCorruptedDataHandling object at 0x7f2b1676f690>\n\n    def test_malformed_configuration_data(self):\n        \"\"\"Test handling of malformed configuration data.\"\"\"\n        from retileup.utils.validation import ValidationUtils\n        from pydantic import BaseModel\n    \n        class TestConfig(BaseModel):\n            name: str\n            value: int\n    \n        # Test with malformed data\n        malformed_data = [\n            {\"name\": \"test\"},  # Missing required field\n            {\"name\": \"test\", \"value\": \"not_an_int\"},  # Wrong type\n            {\"name\": \"\", \"value\": -1},  # Invalid values\n            {},  # Empty data\n        ]\n    \n        for data in malformed_data:\n            result = ValidationUtils.validate_pydantic_model(data, TestConfig)\n>           assert not result.is_valid\nE           assert not True\nE            +  where True = <retileup.utils.validation.ValidationResult object at 0x7f2b16044550>.is_valid\n\ntests/edge_cases/test_edge_cases.py:514: AssertionError\n____________ TestCorruptedDataHandling.test_invalid_coordinate_data ____________\n\nself = <test_edge_cases.TestCorruptedDataHandling object at 0x7f2b1676fcd0>\n\n    def test_invalid_coordinate_data(self):\n        \"\"\"Test handling of invalid coordinate data.\"\"\"\n        invalid_coordinates = [\n            \"not a list\",\n            [1, 2, 3],  # Not tuples\n            [(1,)],  # Single element tuples\n            [(1, 2, 3)],  # Too many elements\n            [(\"a\", \"b\")],  # Non-numeric\n            [(1.5, float('inf'))],  # Infinite values\n            [(1, float('nan'))],  # NaN values\n        ]\n    \n        for coords in invalid_coordinates:\n>           assert validate_coordinates(coords) is False\nE           assert True is False\nE            +  where True = validate_coordinates([(1.5, inf)])\n\ntests/edge_cases/test_edge_cases.py:529: AssertionError\n_____________ TestExtremeInputValues.test_extremely_large_numbers ______________\n\nself = <test_edge_cases.TestExtremeInputValues object at 0x7f2b16775290>\n\n    def test_extremely_large_numbers(self):\n        \"\"\"Test handling of extremely large numbers.\"\"\"\n        large_numbers = [\n            sys.maxsize,\n            sys.maxsize * 2,\n            float('inf'),\n            1e100,\n        ]\n    \n        for num in large_numbers:\n            if num == float('inf'):\n                # Infinity should be rejected\n>               assert validate_positive_number(num) is False\nE               assert True is False\nE                +  where True = validate_positive_number(inf)\n\ntests/edge_cases/test_edge_cases.py:592: AssertionError\n__________________ TestCLIBasics.test_cli_no_args_shows_help ___________________\n\nself = <test_cli.TestCLIBasics object at 0x7f2b164bc950>\ncli_runner = <typer.testing.CliRunner object at 0x7f2b1559fb10>\n\n    def test_cli_no_args_shows_help(self, cli_runner):\n        \"\"\"Test that CLI shows help when no arguments provided.\"\"\"\n        result = cli_runner.invoke(app, [])\n    \n>       assert result.exit_code == 0\nE       assert 2 == 0\nE        +  where 2 = <Result SystemExit(2)>.exit_code\n\ntests/integration/test_cli.py:86: AssertionError\n____________ TestCLIConfiguration.test_global_state_initialization _____________\n\nself = <test_cli.TestCLIConfiguration object at 0x7f2b164bf810>\n\n    def test_global_state_initialization(self):\n        \"\"\"Test global state is initialized correctly.\"\"\"\n>       assert global_state.config_file is None\nE       AssertionError: assert PosixPath('/tmp/tmp679w2wso/retileup.yaml') is None\nE        +  where PosixPath('/tmp/tmp679w2wso/retileup.yaml') = <retileup.cli.main.GlobalState object at 0x7f2b168db750>.config_file\n\ntests/integration/test_cli.py:158: AssertionError\n_______________ TestCLIErrorHandling.test_handle_retileup_error ________________\n\nself = <test_cli.TestCLIErrorHandling object at 0x7f2b164c5e50>\n\n    def test_handle_retileup_error(self):\n        \"\"\"Test handling of ReTileUp specific errors.\"\"\"\n        error = ValidationError(\"Test validation error\", error_code=1001)\n>       exit_code = handle_exception(error)\n                    ^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/integration/test_cli.py:219: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/retileup/cli/main.py:227: in handle_exception\n    console.print(f\"[red]Error:[/red] {e}\")\n                  ^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <[AttributeError(\"'int' object has no attribute 'value'\") raised in repr()] ValidationError object at 0x7f2b155f7160>\n\n    def __str__(self) -> str:\n        \"\"\"String representation of the exception.\"\"\"\n>       parts = [f\"{self.error_code.value}: {self.message}\"]\n                    ^^^^^^^^^^^^^^^^^^^^^\nE       AttributeError: 'int' object has no attribute 'value'\n\nsrc/retileup/core/exceptions.py:118: AttributeError\n__________________ TestCLIIntegration.test_cli_callback_order __________________\n\nself = <test_cli.TestCLIIntegration object at 0x7f2b164d2590>\ncli_runner = <typer.testing.CliRunner object at 0x7f2b1549e210>\n\n    def test_cli_callback_order(self, cli_runner):\n        \"\"\"Test that CLI callbacks are executed in correct order.\"\"\"\n        call_order = []\n    \n        def mock_config_callback(ctx, param, value):\n            call_order.append('config')\n            return None\n    \n        def mock_verbose_callback(ctx, param, value):\n            call_order.append('verbose')\n            return value\n    \n        def mock_quiet_callback(ctx, param, value):\n            call_order.append('quiet')\n            return value\n    \n        with patch('retileup.cli.main.config_callback', side_effect=mock_config_callback):\n            with patch('retileup.cli.main.verbose_callback', side_effect=mock_verbose_callback):\n                with patch('retileup.cli.main.quiet_callback', side_effect=mock_quiet_callback):\n                    result = cli_runner.invoke(app, [\n                        \"--config\", \"test.yaml\",\n                        \"--verbose\",\n                        \"--quiet\",\n                        \"hello\"\n                    ])\n    \n                    # Callbacks should be executed\n>                   assert 'config' in call_order\nE                   AssertionError: assert 'config' in []\n\ntests/integration/test_cli.py:342: AssertionError\n_________________ TestCLIIntegration.test_main_execution_path __________________\n\nself = <MagicMock name='handle_exception' id='139823017225104'>\n\n    def assert_called_once(self):\n        \"\"\"assert that the mock was called only once.\n        \"\"\"\n        if not self.call_count == 1:\n            msg = (\"Expected '%s' to have been called once. Called %s times.%s\"\n                   % (self._mock_name or 'mock',\n                      self.call_count,\n                      self._calls_repr()))\n>           raise AssertionError(msg)\nE           AssertionError: Expected 'handle_exception' to have been called once. Called 0 times.\n\n/usr/lib/python3.11/unittest/mock.py:902: AssertionError\n\nDuring handling of the above exception, another exception occurred:\n\nself = <test_cli.TestCLIIntegration object at 0x7f2b164d2410>\n\n    def test_main_execution_path(self):\n        \"\"\"Test main execution path when run as script.\"\"\"\n        with patch('sys.argv', ['retileup', 'hello']):\n            with patch('retileup.cli.main.app') as mock_app:\n                mock_app.side_effect = RuntimeError(\"Test error\")\n    \n                with patch('retileup.cli.main.handle_exception') as mock_handler:\n                    mock_handler.return_value = 42\n    \n                    with patch('sys.exit') as mock_exit:\n                        # This would be the __main__ execution\n                        try:\n                            app()\n                        except Exception as e:\n                            exit_code = handle_exception(e)\n                            # sys.exit(exit_code) would be called\n    \n>                       mock_handler.assert_called_once()\nE                       AssertionError: Expected 'handle_exception' to have been called once. Called 0 times.\n\ntests/integration/test_cli.py:371: AssertionError\n----------------------------- Captured stdout call -----------------------------\n\ud83c\udfa8 ReTileUp CLI is working!\nVersion: 0.1.0\nReady for image processing workflows!\n_______________ TestCLIPerformance.test_cli_concurrent_execution _______________\n\nself = <test_cli.TestCLIPerformance object at 0x7f2b164d2f90>\ncli_runner = <typer.testing.CliRunner object at 0x7f2b1545e690>\n\n    def test_cli_concurrent_execution(self, cli_runner):\n        \"\"\"Test CLI handles concurrent execution correctly.\"\"\"\n        import threading\n        import queue\n    \n        results = queue.Queue()\n        errors = queue.Queue()\n    \n        def run_cli():\n            try:\n                runner = CliRunner()\n                result = runner.invoke(app, [\"hello\"])\n                results.put(result.exit_code)\n            except Exception as e:\n                errors.put(e)\n    \n        # Start multiple CLI instances\n        threads = []\n        for _ in range(3):\n            thread = threading.Thread(target=run_cli)\n            threads.append(thread)\n            thread.start()\n    \n        # Wait for completion\n        for thread in threads:\n            thread.join()\n    \n        # Check results\n>       assert errors.empty(), f\"Errors occurred: {list(errors.queue)}\"\nE       AssertionError: Errors occurred: [ValueError('I/O operation on closed file.'), ValueError('I/O operation on closed file.')]\nE       assert False\nE        +  where False = empty()\nE        +    where empty = <queue.Queue object at 0x7f2b15400410>.empty\n\ntests/integration/test_cli.py:427: AssertionError\n__________________ TestCLIEdgeCases.test_cli_signal_handling ___________________\n\nself = <TyperGroup retileup>\nargs = ['--cov-report=html:htmlcov', '--cov-report=xml:coverage.xml', '--cov-report=term-missing', '--cov-report=json:coverage.json', '--cov-branch', '--cov-fail-under=90', ...]\nprog_name = 'python -m pytest', complete_var = None, standalone_mode = True\nwindows_expand_args = True, rich_markup_mode = 'rich', extra = {}\n\n    def _main(\n        self: click.Command,\n        *,\n        args: Optional[Sequence[str]] = None,\n        prog_name: Optional[str] = None,\n        complete_var: Optional[str] = None,\n        standalone_mode: bool = True,\n        windows_expand_args: bool = True,\n        rich_markup_mode: MarkupMode = DEFAULT_MARKUP_MODE,\n        **extra: Any,\n    ) -> Any:\n        # Typer override, duplicated from click.main() to handle custom rich exceptions\n        # Verify that the environment is configured correctly, or reject\n        # further execution to avoid a broken script.\n        if args is None:\n            args = sys.argv[1:]\n    \n            # Covered in Click tests\n            if os.name == \"nt\" and windows_expand_args:  # pragma: no cover\n                args = click.utils._expand_args(args)\n        else:\n            args = list(args)\n    \n        if prog_name is None:\n            prog_name = click.utils._detect_program_name()\n    \n        # Process shell completion requests and exit early.\n        self._main_shell_completion(extra, prog_name, complete_var)\n    \n        try:\n            try:\n>               with self.make_context(prog_name, args, **extra) as ctx:\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n/home/olereon/.local/lib/python3.11/site-packages/typer/core.py:191: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/home/olereon/.local/lib/python3.11/site-packages/click/core.py:1186: in make_context\n    self.parse_args(ctx, args)\n/home/olereon/.local/lib/python3.11/site-packages/click/core.py:1786: in parse_args\n    rest = super().parse_args(ctx, args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/home/olereon/.local/lib/python3.11/site-packages/click/core.py:1194: in parse_args\n    opts, args, param_order = parser.parse_args(args=args)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/home/olereon/.local/lib/python3.11/site-packages/click/parser.py:307: in parse_args\n    self._process_args_for_options(state)\n/home/olereon/.local/lib/python3.11/site-packages/click/parser.py:334: in _process_args_for_options\n    self._process_opts(arg, state)\n/home/olereon/.local/lib/python3.11/site-packages/click/parser.py:484: in _process_opts\n    self._match_long_opt(norm_long_opt, explicit_value, state)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <click.parser._OptionParser object at 0x7f2b154bf090>, opt = '--cov'\nexplicit_value = 'src/retileup'\nstate = <click.parser._ParsingState object at 0x7f2b154bf790>\n\n    def _match_long_opt(\n        self, opt: str, explicit_value: str | None, state: _ParsingState\n    ) -> None:\n        if opt not in self._long_opt:\n            from difflib import get_close_matches\n    \n            possibilities = get_close_matches(opt, self._long_opt)\n>           raise NoSuchOption(opt, possibilities=possibilities, ctx=self.ctx)\nE           click.exceptions.NoSuchOption: No such option: --cov\n\n/home/olereon/.local/lib/python3.11/site-packages/click/parser.py:368: NoSuchOption\n\nDuring handling of the above exception, another exception occurred:\n\nself = <test_cli.TestCLIEdgeCases object at 0x7f2b162d8d90>\ncli_runner = <typer.testing.CliRunner object at 0x7f2b15479750>\n\n    def test_cli_signal_handling(self, cli_runner):\n        \"\"\"Test CLI signal handling.\"\"\"\n        # Test that KeyboardInterrupt is properly handled\n        with patch('retileup.cli.main.app', side_effect=KeyboardInterrupt()):\n            with patch('retileup.cli.main.handle_exception') as mock_handler:\n                mock_handler.return_value = 130\n    \n                try:\n>                   app()\n\ntests/integration/test_cli.py:489: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n/home/olereon/.local/lib/python3.11/site-packages/typer/main.py:308: in __call__\n    return get_command(self)(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n/home/olereon/.local/lib/python3.11/site-packages/click/core.py:1442: in __call__\n    return self.main(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n/home/olereon/.local/lib/python3.11/site-packages/typer/core.py:784: in main\n    return _main(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <TyperGroup retileup>\nargs = ['--cov-report=html:htmlcov', '--cov-report=xml:coverage.xml', '--cov-report=term-missing', '--cov-report=json:coverage.json', '--cov-branch', '--cov-fail-under=90', ...]\nprog_name = 'python -m pytest', complete_var = None, standalone_mode = True\nwindows_expand_args = True, rich_markup_mode = 'rich', extra = {}\n\n    def _main(\n        self: click.Command,\n        *,\n        args: Optional[Sequence[str]] = None,\n        prog_name: Optional[str] = None,\n        complete_var: Optional[str] = None,\n        standalone_mode: bool = True,\n        windows_expand_args: bool = True,\n        rich_markup_mode: MarkupMode = DEFAULT_MARKUP_MODE,\n        **extra: Any,\n    ) -> Any:\n        # Typer override, duplicated from click.main() to handle custom rich exceptions\n        # Verify that the environment is configured correctly, or reject\n        # further execution to avoid a broken script.\n        if args is None:\n            args = sys.argv[1:]\n    \n            # Covered in Click tests\n            if os.name == \"nt\" and windows_expand_args:  # pragma: no cover\n                args = click.utils._expand_args(args)\n        else:\n            args = list(args)\n    \n        if prog_name is None:\n            prog_name = click.utils._detect_program_name()\n    \n        # Process shell completion requests and exit early.\n        self._main_shell_completion(extra, prog_name, complete_var)\n    \n        try:\n            try:\n                with self.make_context(prog_name, args, **extra) as ctx:\n                    rv = self.invoke(ctx)\n                    if not standalone_mode:\n                        return rv\n                    # it's not safe to `ctx.exit(rv)` here!\n                    # note that `rv` may actually contain data like \"1\" which\n                    # has obvious effects\n                    # more subtle case: `rv=[None, None]` can come out of\n                    # chained commands which all returned `None` -- so it's not\n                    # even always obvious that `rv` indicates success/failure\n                    # by its truthiness/falsiness\n                    ctx.exit()\n            except EOFError as e:\n                click.echo(file=sys.stderr)\n                raise click.Abort() from e\n            except KeyboardInterrupt as e:\n                raise click.exceptions.Exit(130) from e\n            except click.ClickException as e:\n                if not standalone_mode:\n                    raise\n                # Typer override\n                if HAS_RICH and rich_markup_mode is not None:\n                    from . import rich_utils\n    \n                    rich_utils.rich_format_error(e)\n                else:\n                    e.show()\n                # Typer override end\n>               sys.exit(e.exit_code)\nE               SystemExit: 2\n\n/home/olereon/.local/lib/python3.11/site-packages/typer/core.py:219: SystemExit\n----------------------------- Captured stderr call -----------------------------\nUsage: python -m pytest [OPTIONS] COMMAND [ARGS]...\nTry 'python -m pytest -h' for help.\n\u256d\u2500 Error \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 No such option: --cov Did you mean --config?                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n_____ TestEndToEndImageProcessing.test_image_processing_memory_efficiency ______\n\nself = <test_end_to_end.TestEndToEndImageProcessing object at 0x7f2b162f0c10>\nsample_images = {'gray': PosixPath('/tmp/tmpa6_o7dyz/sample_gray.png'), 'large': PosixPath('/tmp/tmpa6_o7dyz/large_image.jpg'), 'rgb': PosixPath('/tmp/tmpa6_o7dyz/sample_rgb.jpg'), 'rgba': PosixPath('/tmp/tmpa6_o7dyz/sample_rgba.png')}\n\n    def test_image_processing_memory_efficiency(self, sample_images):\n        \"\"\"Test image processing memory efficiency.\"\"\"\n        from retileup.utils.image import ImageUtils\n    \n        # Test memory estimation\n        memory_estimate = ImageUtils.estimate_processing_memory(\n            2000, 1500, 16  # Large image with 16 tiles\n        )\n    \n        assert memory_estimate['base_image_mb'] > 0\n        assert memory_estimate['peak_memory_mb'] > memory_estimate['base_image_mb']\n>       assert memory_estimate['processing_feasible'] in [True, False]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       KeyError: 'processing_feasible'\n\ntests/integration/test_end_to_end.py:246: KeyError\n________________ TestTilingIntegration.test_large_image_tiling _________________\n\nself = <test_tiling_integration.TestTilingIntegration object at 0x7f2b162fd0d0>\ntemp_output_dir = PosixPath('/tmp/pytest-of-olereon/pytest-1/test_large_image_tiling0/integration_output')\n\n    def test_large_image_tiling(self, temp_output_dir):\n        \"\"\"Test tiling of a large image with many tiles.\"\"\"\n        # Create a large test image\n        large_image = Image.new('RGB', (1600, 1200), color=(100, 150, 200))\n    \n        # Add some visual pattern for testing\n        for x in range(0, 1600, 100):\n            for y in range(0, 1200, 100):\n                # Create small colored rectangles\n                for i in range(50):\n                    for j in range(50):\n                        if x + i < 1600 and y + j < 1200:\n                            large_image.putpixel((x + i, y + j), (\n                                (x // 10) % 255,\n                                (y // 10) % 255,\n                                ((x + y) // 20) % 255\n                            ))\n    \n        large_image_path = temp_output_dir / \"large_image.png\"\n        large_image.save(large_image_path)\n    \n        tool = TilingTool()\n    \n        # Generate grid of tile coordinates\n        tile_size = 256\n        coordinates = []\n        for x in range(0, 1600 - tile_size, tile_size):\n            for y in range(0, 1200 - tile_size, tile_size):\n                coordinates.append((x, y))\n    \n>       config = TilingConfig(\n            input_path=large_image_path,\n            output_dir=temp_output_dir / \"tiles\",\n            tile_width=tile_size,\n            tile_height=tile_size,\n            coordinates=coordinates,\n            output_pattern=\"grid_{x}_{y}.png\"\n        )\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for TilingConfig\nE       output_pattern\nE         Value error, Output pattern must contain {base} placeholder [type=value_error, input_value='grid_{x}_{y}.png', input_type=str]\nE           For further information visit https://errors.pydantic.dev/2.11/v/value_error\n\ntests/integration/test_tiling_integration.py:102: ValidationError\n_________________ TestTilingIntegration.test_concurrent_safety _________________\n\nself = <test_tiling_integration.TestTilingIntegration object at 0x7f2b1630ac90>\nsample_image_path = PosixPath('/tmp/pytest-of-olereon/pytest-1/test_concurrent_safety0/integration_test_image.jpg')\ntemp_output_dir = PosixPath('/tmp/pytest-of-olereon/pytest-1/test_concurrent_safety0/integration_output')\n\n    def test_concurrent_safety(self, sample_image_path, temp_output_dir):\n        \"\"\"Test that multiple tool instances can run concurrently.\"\"\"\n        import threading\n        import concurrent.futures\n    \n        def run_tiling(thread_id):\n            \"\"\"Run tiling in a separate thread.\"\"\"\n            tool = TilingTool()\n            thread_output_dir = temp_output_dir / f\"thread_{thread_id}\"\n            thread_output_dir.mkdir()\n    \n            config = TilingConfig(\n                input_path=sample_image_path,\n                output_dir=thread_output_dir,\n                tile_width=100,\n                tile_height=100,\n                coordinates=[(thread_id * 50, 0)],\n                output_pattern=f\"thread_{thread_id}_{{x}}_{{y}}.{{ext}}\"\n            )\n    \n            return tool.execute(config)\n    \n        # Run multiple tiling operations concurrently\n        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n            futures = [executor.submit(run_tiling, i) for i in range(3)]\n>           results = [future.result() for future in futures]\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/integration/test_tiling_integration.py:264: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \ntests/integration/test_tiling_integration.py:264: in <listcomp>\n    results = [future.result() for future in futures]\n               ^^^^^^^^^^^^^^^\n/usr/lib/python3.11/concurrent/futures/_base.py:449: in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n/usr/lib/python3.11/concurrent/futures/_base.py:401: in __get_result\n    raise self._exception\n/usr/lib/python3.11/concurrent/futures/thread.py:58: in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nthread_id = 0\n\n    def run_tiling(thread_id):\n        \"\"\"Run tiling in a separate thread.\"\"\"\n        tool = TilingTool()\n        thread_output_dir = temp_output_dir / f\"thread_{thread_id}\"\n        thread_output_dir.mkdir()\n    \n>       config = TilingConfig(\n            input_path=sample_image_path,\n            output_dir=thread_output_dir,\n            tile_width=100,\n            tile_height=100,\n            coordinates=[(thread_id * 50, 0)],\n            output_pattern=f\"thread_{thread_id}_{{x}}_{{y}}.{{ext}}\"\n        )\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for TilingConfig\nE       output_pattern\nE         Value error, Output pattern must contain {base} placeholder [type=value_error, input_value='thread_0_{x}_{y}.{ext}', input_type=str]\nE           For further information visit https://errors.pydantic.dev/2.11/v/value_error\n\ntests/integration/test_tiling_integration.py:250: ValidationError\n___________ TestImageProcessingPerformance.test_large_image_handling ___________\n\nself = <test_performance.TestImageProcessingPerformance object at 0x7f2b161df090>\nperformance_images = {'large': (PosixPath('/tmp/tmp2zkkg4zc/large.png'), 1000, 1000), 'medium': (PosixPath('/tmp/tmp2zkkg4zc/medium.png'), ...ath('/tmp/tmp2zkkg4zc/small.png'), 100, 100), 'very_large': (PosixPath('/tmp/tmp2zkkg4zc/very_large.png'), 2000, 2000)}\nmemory_tracker = <test_performance.memory_tracker.<locals>.MemoryTracker object at 0x7f2b1548bb90>\n\n    @pytest.mark.skipif(sys.maxsize <= 2**32, reason=\"Requires 64-bit system\")\n    def test_large_image_handling(self, performance_images, memory_tracker):\n        \"\"\"Test handling of very large images.\"\"\"\n        if 'very_large' not in performance_images:\n            pytest.skip(\"Very large image not available\")\n    \n        memory_tracker.start()\n        large_path, width, height = performance_images['very_large']\n    \n        start_time = time.time()\n    \n        # Load large image\n        image = ImageUtils.load_image(large_path)\n        memory_tracker.update()\n    \n        # Perform basic operations\n        info = ImageUtils.get_image_info(image)\n        memory_tracker.update()\n    \n        # Resize to smaller size\n        resized = ImageUtils.resize_image(image, (500, 500))\n        memory_tracker.update()\n    \n        end_time = time.time()\n        total_time = end_time - start_time\n    \n        memory_stats = memory_tracker.stop()\n    \n        # Performance assertions for large images\n        assert total_time < 30.0  # Should complete within 30 seconds\n        assert info['width'] == width\n        assert info['height'] == height\n        assert resized.size == (500, 500)\n    \n        # Memory usage should be controlled even for large images\n        expected_memory_mb = (width * height * 3) / (1024 * 1024)  # RGB bytes to MB\n>       assert memory_stats['peak_mb'] < expected_memory_mb * 3  # Allow 3x overhead\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       assert 144.37890625 < (11.444091796875 * 3)\n\ntests/performance/test_performance.py:336: AssertionError\n_______________ TestErrorCode.test_error_code_string_comparison ________________\n\nself = <test_core.test_exceptions.TestErrorCode object at 0x7f2b1621a510>\n\n    def test_error_code_string_comparison(self):\n        \"\"\"Test error code string comparison.\"\"\"\n        assert ErrorCode.VALIDATION_ERROR == \"1000\"\n>       assert str(ErrorCode.VALIDATION_ERROR) == \"1000\"\nE       AssertionError: assert 'ErrorCode.VALIDATION_ERROR' == '1000'\nE         \nE         - 1000\nE         + ErrorCode.VALIDATION_ERROR\n\ntests/unit/test_core/test_exceptions.py:53: AssertionError\n__________________ TestErrorChaining.test_nested_error_chain ___________________\n\nself = <test_core.test_exceptions.TestErrorChaining object at 0x7f2b16232290>\n\n    def test_nested_error_chain(self):\n        \"\"\"Test nested error chaining.\"\"\"\n        # Create a chain: ValueError -> ProcessingError -> ValidationError\n        root_cause = ValueError(\"Original error\")\n        processing_error = ProcessingError(\n            \"Processing wrapper\",\n            cause=root_cause\n        )\n        validation_error = ValidationError(\n            \"Validation wrapper\",\n            cause=processing_error\n        )\n    \n        assert validation_error.cause == processing_error\n        assert processing_error.cause == root_cause\n    \n        # Check string representation includes cause\n        validation_str = str(validation_error)\n>       assert \"Caused by: Processing wrapper\" in validation_str\nE       AssertionError: assert 'Caused by: Processing wrapper' in '1000: Validation wrapper | Caused by: 2000: Processing wrapper | Caused by: Original error'\n\ntests/unit/test_core/test_exceptions.py:528: AssertionError\n____________ TestErrorChaining.test_error_serialization_with_cause _____________\n\nself = <test_core.test_exceptions.TestErrorChaining object at 0x7f2b162328d0>\n\n    def test_error_serialization_with_cause(self):\n        \"\"\"Test error serialization with cause chain.\"\"\"\n        root_cause = KeyError(\"Missing key\")\n        wrapped_error = ConfigurationError(\n            \"Config error\",\n            cause=root_cause\n        )\n    \n        error_dict = wrapped_error.to_dict()\n    \n        assert error_dict[\"error_type\"] == \"ConfigurationError\"\n        assert error_dict[\"message\"] == \"Config error\"\n        assert error_dict[\"cause\"][\"type\"] == \"KeyError\"\n>       assert error_dict[\"cause\"][\"message\"] == \"Missing key\"\nE       assert \"'Missing key'\" == 'Missing key'\nE         \nE         - Missing key\nE         + 'Missing key'\nE         ? +           +\n\ntests/unit/test_core/test_exceptions.py:543: AssertionError\n_____________ TestToolRegistry.test_tool_registration_failing_tool _____________\n\nself = <test_core.test_registry.TestToolRegistry object at 0x7f2b16255610>\n\n    def test_tool_registration_failing_tool(self):\n        \"\"\"Test registration of tool with invalid properties.\"\"\"\n        registry = ToolRegistry()\n    \n        with pytest.raises(RegistryError) as exc_info:\n            registry.register_tool(MockFailingTool)\n    \n>       assert \"failed validation\" in str(exc_info.value)\nE       AssertionError: assert 'failed validation' in '3002: Failed to register tool MockFailingTool: 3002: Tool MockFailingTool must have valid description property | Context: tool_name=<property object at 0x7f2b1620b510>, registry_operation=register | Caused by: 3002: Tool MockFailingTool must have valid description property'\nE        +  where '3002: Failed to register tool MockFailingTool: 3002: Tool MockFailingTool must have valid description property | Context: tool_name=<property object at 0x7f2b1620b510>, registry_operation=register | Caused by: 3002: Tool MockFailingTool must have valid description property' = str(<RegistryError(error_code='3002', message='Failed to register tool MockFailingTool: 3002: Tool MockFailingTool must have valid description property', context={'tool_name': <property object at 0x7f2b1620b510>, 'registry_operation': 'register'})>)\nE        +    where <RegistryError(error_code='3002', message='Failed to register tool MockFailingTool: 3002: Tool MockFailingTool must have valid description property', context={'tool_name': <property object at 0x7f2b1620b510>, 'registry_operation': 'register'})> = <ExceptionInfo <RegistryError(error_code='3002', message='Failed to register tool MockFailingTool: 3002: Tool MockFailingTool must ha...d description property', context={'tool_name': <property object at 0x7f2b1620b510>, 'registry_operation': 'register'})> tblen=2>.value\n\ntests/unit/test_core/test_registry.py:211: AssertionError\n______________ TestToolRegistry.test_create_tool_instance_failure ______________\n\nself = <FailingInstantiationTool(name='failing-instantiation', version='1.0.0')>\n\n    def __init__(self):\n>       raise RuntimeError(\"Failed to instantiate\")\nE       RuntimeError: Failed to instantiate\n\ntests/unit/test_core/test_registry.py:277: RuntimeError\n\nThe above exception was the direct cause of the following exception:\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <class 'test_core.test_registry.TestToolRegistry.test_create_tool_instance_failure.<locals>.FailingInstantiationTool'>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <class 'test_core.test_registry.TestToolRegistry.test_create_tool_instance_failure.<locals>.FailingInstantiationTool'>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n        if not issubclass(tool_class, BaseTool):\n            raise RegistryError(\n                f\"Tool class {tool_class.__name__} must inherit from BaseTool\",\n                error_code=ErrorCode.TOOL_REGISTRATION_ERROR,\n            )\n    \n        # Check if it's not the abstract base class\n        if tool_class is BaseTool:\n            raise RegistryError(\n                \"Cannot register abstract BaseTool class\",\n                error_code=ErrorCode.TOOL_REGISTRATION_ERROR,\n            )\n    \n        # Try to instantiate and check required methods\n        try:\n            instance = tool_class()\n    \n            # Check required properties\n            required_props = [\"name\", \"description\", \"version\"]\n            for prop in required_props:\n                value = getattr(instance, prop)\n                if not value or not isinstance(value, str):\n                    raise RegistryError(\n                        f\"Tool {tool_class.__name__} must have valid {prop} property\",\n                        error_code=ErrorCode.TOOL_REGISTRATION_ERROR,\n                    )\n    \n            # Check required methods\n            required_methods = [\"get_config_schema\", \"validate_config\", \"execute\"]\n            for method in required_methods:\n                if not hasattr(instance, method) or not callable(getattr(instance, method)):\n                    raise RegistryError(\n                        f\"Tool {tool_class.__name__} must implement {method} method\",\n                        error_code=ErrorCode.TOOL_REGISTRATION_ERROR,\n                    )\n    \n        except Exception as e:\n            if isinstance(e, RegistryError):\n                raise\n>           raise RegistryError(\n                f\"Tool {tool_class.__name__} failed validation: {str(e)}\",\n                error_code=ErrorCode.TOOL_REGISTRATION_ERROR,\n                cause=e,\n            ) from e\nE           retileup.core.exceptions.RegistryError: 3002: Tool FailingInstantiationTool failed validation: Failed to instantiate | Caused by: Failed to instantiate\n\nsrc/retileup/core/registry.py:243: RegistryError\n\nThe above exception was the direct cause of the following exception:\n\nself = <test_core.test_registry.TestToolRegistry object at 0x7f2b16257b50>\n\n    def test_create_tool_instance_failure(self):\n        \"\"\"Test creating tool instance that fails during instantiation.\"\"\"\n        registry = ToolRegistry()\n    \n        class FailingInstantiationTool(BaseTool):\n            def __init__(self):\n                raise RuntimeError(\"Failed to instantiate\")\n    \n            @property\n            def name(self) -> str:\n                return \"failing-instantiation\"\n    \n            @property\n            def description(self) -> str:\n                return \"Fails during instantiation\"\n    \n            @property\n            def version(self) -> str:\n                return \"1.0.0\"\n    \n            def get_config_schema(self) -> Type[ToolConfig]:\n                return ToolConfig\n    \n            def validate_config(self, config: ToolConfig) -> List[str]:\n                return []\n    \n            def execute(self, config: ToolConfig) -> ToolResult:\n                return ToolResult(success=True, message=\"Should not reach here\")\n    \n        # Registration should work (we don't instantiate during registration)\n>       registry.register_tool(FailingInstantiationTool)\n\ntests/unit/test_core/test_registry.py:301: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <class 'test_core.test_registry.TestToolRegistry.test_create_tool_instance_failure.<locals>.FailingInstantiationTool'>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n>               raise registry_error(\n                    f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               retileup.core.exceptions.RegistryError: 3002: Failed to register tool FailingInstantiationTool: 3002: Tool FailingInstantiationTool failed validation: Failed to instantiate | Caused by: Failed to instantiate | Context: tool_name=<property object at 0x7f2b15427790>, registry_operation=register | Caused by: 3002: Tool FailingInstantiationTool failed validation: Failed to instantiate | Caused by: Failed to instantiate\n\nsrc/retileup/core/registry.py:187: RegistryError\n____ TestToolRegistryHealthAndValidation.test_validate_tool_health_failure _____\n\nself = <FailingHealthTool(name='failing-health', version='1.0.0')>\n\n    def __init__(self):\n>       raise RuntimeError(\"Health check failure\")\nE       RuntimeError: Health check failure\n\ntests/unit/test_core/test_registry.py:649: RuntimeError\n\nThe above exception was the direct cause of the following exception:\n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <class 'test_core.test_registry.TestToolRegistryHealthAndValidation.test_validate_tool_health_failure.<locals>.FailingHealthTool'>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n>               self._validate_tool_class(tool_class)\n\nsrc/retileup/core/registry.py:150: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <class 'test_core.test_registry.TestToolRegistryHealthAndValidation.test_validate_tool_health_failure.<locals>.FailingHealthTool'>\n\n    def _validate_tool_class(self, tool_class: Type[BaseTool]) -> None:\n        \"\"\"Validate that a tool class meets requirements.\n    \n        Args:\n            tool_class: Tool class to validate\n    \n        Raises:\n            RegistryError: If validation fails\n        \"\"\"\n        # Check if it's a subclass of BaseTool\n        if not issubclass(tool_class, BaseTool):\n            raise RegistryError(\n                f\"Tool class {tool_class.__name__} must inherit from BaseTool\",\n                error_code=ErrorCode.TOOL_REGISTRATION_ERROR,\n            )\n    \n        # Check if it's not the abstract base class\n        if tool_class is BaseTool:\n            raise RegistryError(\n                \"Cannot register abstract BaseTool class\",\n                error_code=ErrorCode.TOOL_REGISTRATION_ERROR,\n            )\n    \n        # Try to instantiate and check required methods\n        try:\n            instance = tool_class()\n    \n            # Check required properties\n            required_props = [\"name\", \"description\", \"version\"]\n            for prop in required_props:\n                value = getattr(instance, prop)\n                if not value or not isinstance(value, str):\n                    raise RegistryError(\n                        f\"Tool {tool_class.__name__} must have valid {prop} property\",\n                        error_code=ErrorCode.TOOL_REGISTRATION_ERROR,\n                    )\n    \n            # Check required methods\n            required_methods = [\"get_config_schema\", \"validate_config\", \"execute\"]\n            for method in required_methods:\n                if not hasattr(instance, method) or not callable(getattr(instance, method)):\n                    raise RegistryError(\n                        f\"Tool {tool_class.__name__} must implement {method} method\",\n                        error_code=ErrorCode.TOOL_REGISTRATION_ERROR,\n                    )\n    \n        except Exception as e:\n            if isinstance(e, RegistryError):\n                raise\n>           raise RegistryError(\n                f\"Tool {tool_class.__name__} failed validation: {str(e)}\",\n                error_code=ErrorCode.TOOL_REGISTRATION_ERROR,\n                cause=e,\n            ) from e\nE           retileup.core.exceptions.RegistryError: 3002: Tool FailingHealthTool failed validation: Health check failure | Caused by: Health check failure\n\nsrc/retileup/core/registry.py:243: RegistryError\n\nThe above exception was the direct cause of the following exception:\n\nself = <test_core.test_registry.TestToolRegistryHealthAndValidation object at 0x7f2b16268c50>\n\n    def test_validate_tool_health_failure(self):\n        \"\"\"Test health validation for failing tool.\"\"\"\n        registry = ToolRegistry()\n    \n        class FailingHealthTool(BaseTool):\n            def __init__(self):\n                raise RuntimeError(\"Health check failure\")\n    \n            @property\n            def name(self) -> str:\n                return \"failing-health\"\n    \n            @property\n            def description(self) -> str:\n                return \"Fails health check\"\n    \n            @property\n            def version(self) -> str:\n                return \"1.0.0\"\n    \n            def get_config_schema(self) -> Type[ToolConfig]:\n                return ToolConfig\n    \n            def validate_config(self, config: ToolConfig) -> List[str]:\n                return []\n    \n            def execute(self, config: ToolConfig) -> ToolResult:\n                return ToolResult(success=True, message=\"Should not reach here\")\n    \n>       registry.register_tool(FailingHealthTool)\n\ntests/unit/test_core/test_registry.py:672: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <ToolRegistry(tools=0, directories=1)>\ntool_class = <class 'test_core.test_registry.TestToolRegistryHealthAndValidation.test_validate_tool_health_failure.<locals>.FailingHealthTool'>\nname = None, force = False\n\n    def register_tool(\n        self,\n        tool_class: Type[BaseTool],\n        name: Optional[str] = None,\n        force: bool = False,\n    ) -> None:\n        \"\"\"Register a tool class with enhanced validation.\n    \n        Args:\n            tool_class: The tool class to register\n            name: Optional custom name for the tool\n            force: Whether to force registration even if tool exists\n    \n        Raises:\n            RegistryError: If registration fails or tool validation fails\n        \"\"\"\n        with self._lock:\n            try:\n                # Validate the tool class\n                self._validate_tool_class(tool_class)\n    \n                # Create instance to get metadata\n                tool_instance = tool_class()\n                tool_name = name or tool_instance.name\n                tool_version = tool_instance.version\n                tool_description = tool_instance.description\n    \n                # Check for existing registration\n                if tool_name in self._tools and not force:\n                    existing_metadata = self._tools[tool_name]\n                    logger.warning(\n                        f\"Tool '{tool_name}' already registered \"\n                        f\"(existing: v{existing_metadata.version}, \"\n                        f\"new: v{tool_version}). Use force=True to override.\"\n                    )\n                    return\n    \n                # Create metadata\n                metadata = ToolMetadata(\n                    tool_class=tool_class,\n                    name=tool_name,\n                    version=tool_version,\n                    description=tool_description,\n                    registration_time=time.time(),\n                    source_module=tool_class.__module__,\n                )\n    \n                # Register the tool\n                self._tools[tool_name] = metadata\n    \n                logger.info(\n                    f\"Registered tool: {tool_name} v{tool_version} \"\n                    f\"from {tool_class.__module__}\"\n                )\n    \n            except Exception as e:\n>               raise registry_error(\n                    f\"Failed to register tool {tool_class.__name__}: {str(e)}\",\n                    tool_name=getattr(tool_class, \"name\", tool_class.__name__),\n                    operation=\"register\",\n                    cause=e,\n                ) from e\nE               retileup.core.exceptions.RegistryError: 3002: Failed to register tool FailingHealthTool: 3002: Tool FailingHealthTool failed validation: Health check failure | Caused by: Health check failure | Context: tool_name=<property object at 0x7f2b153fe890>, registry_operation=register | Caused by: 3002: Tool FailingHealthTool failed validation: Health check failure | Caused by: Health check failure\n\nsrc/retileup/core/registry.py:187: RegistryError\n_____________ TestToolRegistryThreadSafety.test_concurrent_access ______________\n\nself = <test_core.test_registry.TestToolRegistryThreadSafety object at 0x7f2b16269b10>\n\n    def test_concurrent_access(self):\n        \"\"\"Test concurrent tool access.\"\"\"\n        registry = ToolRegistry()\n        registry.register_tool(MockValidTool)\n    \n        access_results = []\n        access_errors = []\n    \n        def access_tool():\n            try:\n                for _ in range(100):\n                    tool_class = registry.get_tool_class(\"mock-valid-tool\")\n                    assert tool_class == MockValidTool\n    \n                    tool_instance = registry.create_tool(\"mock-valid-tool\")\n                    assert isinstance(tool_instance, MockValidTool)\n    \n                access_results.append(True)\n            except Exception as e:\n                access_errors.append(e)\n    \n        # Start multiple threads\n        threads = []\n        for _ in range(5):\n            thread = threading.Thread(target=access_tool)\n            threads.append(thread)\n            thread.start()\n    \n        # Wait for all threads to complete\n        for thread in threads:\n            thread.join()\n    \n        # Check results\n        assert len(access_errors) == 0, f\"Unexpected errors: {access_errors}\"\n        assert len(access_results) == 5\n    \n        # Check usage count was updated correctly\n        metadata = registry.get_tool_metadata(\"mock-valid-tool\")\n>       assert metadata[\"usage_count\"] == 5 * 100  # 5 threads \u00d7 100 accesses each\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nE       assert 1000 == (5 * 100)\n\ntests/unit/test_core/test_registry.py:759: AssertionError\n____________ TestToolEdgeCases.test_tool_execution_with_none_config ____________\n\nself = <test_tools.test_base.TestToolEdgeCases object at 0x7f2b162a4c50>\n\n    def test_tool_execution_with_none_config(self):\n        \"\"\"Test tool behavior with None configuration.\"\"\"\n        tool = ConcreteTestTool()\n    \n        # This should raise an appropriate error\n>       with pytest.raises((TypeError, AttributeError)):\nE       Failed: DID NOT RAISE any of (<class 'TypeError'>, <class 'AttributeError'>)\n\ntests/unit/test_tools/test_base.py:534: Failed\n________________ TestToolEdgeCases.test_tool_memory_efficiency _________________\n\nself = <test_tools.test_base.TestToolEdgeCases object at 0x7f2b162a5290>\ntemp_dir = PosixPath('/tmp/tmpzqad9ay4')\nmemory_monitor = <tests.conftest.memory_monitor.<locals>.MemoryMonitor object at 0x7f2b150af350>\n\n    def test_tool_memory_efficiency(self, temp_dir, memory_monitor):\n        \"\"\"Test that tool execution doesn't leak memory.\"\"\"\n        tool = ConcreteTestTool()\n        config = ToolConfig(input_path=temp_dir / \"test.jpg\")\n    \n        memory_monitor.start()\n    \n        # Execute tool many times\n        for _ in range(100):\n            result = tool.execute_with_timing(config)\n            assert result.success\n    \n        peak_memory = memory_monitor.stop()\n    \n        # Memory usage should be reasonable (less than 100MB for this simple test)\n>       assert peak_memory < 100, f\"Memory usage too high: {peak_memory}MB\"\nE       AssertionError: Memory usage too high: 222.83203125MB\nE       assert 222.83203125 < 100\n\ntests/unit/test_tools/test_base.py:552: AssertionError\n________________ TestTilingConfig.test_invalid_tile_dimensions _________________\n\nself = <test_tools.test_tiling.TestTilingConfig object at 0x7f2b162a7150>\nsample_image_path = PosixPath('/tmp/pytest-of-olereon/pytest-1/test_invalid_tile_dimensions0/test_image.jpg')\ntemp_output_dir = PosixPath('/tmp/pytest-of-olereon/pytest-1/test_invalid_tile_dimensions0/output')\n\n    def test_invalid_tile_dimensions(self, sample_image_path, temp_output_dir):\n        \"\"\"Test validation of invalid tile dimensions.\"\"\"\n        with pytest.raises(pydantic.ValidationError) as exc_info:\n            TilingConfig(\n                input_path=sample_image_path,\n                output_dir=temp_output_dir,\n                tile_width=0,  # Invalid: must be > 0\n                tile_height=100,\n                coordinates=[(0, 0)]\n            )\n    \n        assert \"greater than 0\" in str(exc_info.value)\n    \n        with pytest.raises(pydantic.ValidationError) as exc_info:\n            TilingConfig(\n                input_path=sample_image_path,\n                output_dir=temp_output_dir,\n                tile_width=10000,  # Invalid: too large\n                tile_height=100,\n                coordinates=[(0, 0)]\n            )\n    \n>       assert \"cannot exceed\" in str(exc_info.value)\nE       AssertionError: assert 'cannot exceed' in '1 validation error for TilingConfig\\ntile_width\\n  Input should be less than or equal to 8192 [type=less_than_equal, input_value=10000, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.11/v/less_than_equal'\nE        +  where '1 validation error for TilingConfig\\ntile_width\\n  Input should be less than or equal to 8192 [type=less_than_equal, input_value=10000, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.11/v/less_than_equal' = str(1 validation error for TilingConfig\\ntile_width\\n  Input should be less than or equal to 8192 [type=less_than_equal, input_value=10000, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.11/v/less_than_equal)\nE        +    where 1 validation error for TilingConfig\\ntile_width\\n  Input should be less than or equal to 8192 [type=less_than_equal, input_value=10000, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.11/v/less_than_equal = <ExceptionInfo 1 validation error for TilingConfig\\ntile_width\\n  Input should be less than or equal to 8192 [type=less_than_equal, input_value=10000, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.11/v/less_than_equal tblen=2>.value\n\ntests/unit/test_tools/test_tiling.py:68: AssertionError\n__________________ TestTilingConfig.test_invalid_coordinates ___________________\n\nself = <test_tools.test_tiling.TestTilingConfig object at 0x7f2b162a7510>\nsample_image_path = PosixPath('/tmp/pytest-of-olereon/pytest-1/test_invalid_coordinates0/test_image.jpg')\ntemp_output_dir = PosixPath('/tmp/pytest-of-olereon/pytest-1/test_invalid_coordinates0/output')\n\n    def test_invalid_coordinates(self, sample_image_path, temp_output_dir):\n        \"\"\"Test validation of invalid coordinates.\"\"\"\n        # Empty coordinates\n        with pytest.raises(pydantic.ValidationError) as exc_info:\n            TilingConfig(\n                input_path=sample_image_path,\n                output_dir=temp_output_dir,\n                tile_width=100,\n                tile_height=100,\n                coordinates=[]\n            )\n    \n>       assert \"at least one coordinate\" in str(exc_info.value)\nE       AssertionError: assert 'at least one coordinate' in '1 validation error for TilingConfig\\ncoordinates\\n  List should have at least 1 item after validation, not 0 [type=too_short, input_value=[], input_type=list]\\n    For further information visit https://errors.pydantic.dev/2.11/v/too_short'\nE        +  where '1 validation error for TilingConfig\\ncoordinates\\n  List should have at least 1 item after validation, not 0 [type=too_short, input_value=[], input_type=list]\\n    For further information visit https://errors.pydantic.dev/2.11/v/too_short' = str(1 validation error for TilingConfig\\ncoordinates\\n  List should have at least 1 item after validation, not 0 [type=too_short, input_value=[], input_type=list]\\n    For further information visit https://errors.pydantic.dev/2.11/v/too_short)\nE        +    where 1 validation error for TilingConfig\\ncoordinates\\n  List should have at least 1 item after validation, not 0 [type=too_short, input_value=[], input_type=list]\\n    For further information visit https://errors.pydantic.dev/2.11/v/too_short = <ExceptionInfo 1 validation error for TilingConfig\\ncoordinates\\n  List should have at least 1 item after validation, not 0 [type=too_short, input_value=[], input_type=list]\\n    For further information visit https://errors.pydantic.dev/2.11/v/too_short tblen=2>.value\n\ntests/unit/test_tools/test_tiling.py:82: AssertionError\n______________ TestTilingToolFormats.test_rgba_to_rgb_conversion _______________\n\nself = <test_tools.test_tiling.TestTilingToolFormats object at 0x7f2b162c2c50>\nsample_rgba_image = PosixPath('/tmp/pytest-of-olereon/pytest-1/test_rgba_to_rgb_conversion0/test_rgba.png')\ntemp_output_dir = PosixPath('/tmp/pytest-of-olereon/pytest-1/test_rgba_to_rgb_conversion0/output')\n\n    def test_rgba_to_rgb_conversion(self, sample_rgba_image, temp_output_dir):\n        \"\"\"Test RGBA to RGB conversion for JPEG output.\"\"\"\n        tool = TilingTool()\n>       config = TilingConfig(\n            input_path=sample_rgba_image,\n            output_dir=temp_output_dir,\n            tile_width=100,\n            tile_height=100,\n            coordinates=[(0, 0)],\n            output_pattern=\"{base}_{x}_{y}.jpg\"  # Force JPEG output\n        )\nE       pydantic_core._pydantic_core.ValidationError: 1 validation error for TilingConfig\nE       output_pattern\nE         Value error, Output pattern must contain {ext} placeholder [type=value_error, input_value='{base}_{x}_{y}.jpg', input_type=str]\nE           For further information visit https://errors.pydantic.dev/2.11/v/value_error\n\ntests/unit/test_tools/test_tiling.py:605: ValidationError\n________ TestFileValidation.test_validate_file_path_factory_permissions ________\n\nself = <test_utils.test_validation.TestFileValidation object at 0x7f2b1614fcd0>\ntemp_dir = PosixPath('/tmp/tmpt4op0wgj')\n\n    def test_validate_file_path_factory_permissions(self, temp_dir):\n        \"\"\"Test file path validator with permission checks.\"\"\"\n        test_file = temp_dir / \"test.txt\"\n        test_file.write_text(\"test content\")\n    \n        # Test readable requirement\n        validator_readable = validate_file_path(readable=True)\n>       assert validator_readable(test_file) is True\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntests/unit/test_utils/test_validation.py:248: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\npath = PosixPath('/tmp/tmpt4op0wgj/test.txt')\n\n    def validator(path: Union[str, Path]) -> bool:\n        try:\n            path = Path(path)\n    \n            if must_exist and not path.exists():\n                return False\n    \n            if path.exists():\n                if must_be_file and not path.is_file():\n                    return False\n    \n                # Check permissions on existing files\n>               if readable and not path.readable():\n                                    ^^^^^^^^^^^^^\nE               AttributeError: 'PosixPath' object has no attribute 'readable'\n\nsrc/retileup/utils/validation.py:190: AttributeError\n______ TestFileValidation.test_validate_image_file_factory_error_handling ______\n\nself = <test_utils.test_validation.TestFileValidation object at 0x7f2b16151d10>\ntemp_dir = PosixPath('/tmp/tmpoyks6u37')\n\n    def test_validate_image_file_factory_error_handling(self, temp_dir):\n        \"\"\"Test image file validator error handling.\"\"\"\n        validator = validate_image_file()\n    \n        # Non-existent file\n        assert validator(temp_dir / \"nonexistent.jpg\") is False\n    \n        # Invalid image file\n        invalid_file = temp_dir / \"invalid.jpg\"\n        invalid_file.write_text(\"not an image\")\n>       assert validator(invalid_file) is False\nE       AssertionError: assert True is False\nE        +  where True = <function validate_image_file.<locals>.validator at 0x7f2b151f6840>(PosixPath('/tmp/tmpoyks6u37/invalid.jpg'))\n\ntests/unit/test_utils/test_validation.py:302: AssertionError\n_______ TestValidationUtils.test_validate_directory_path_creation_error ________\n\nself = <test_utils.test_validation.TestValidationUtils object at 0x7f2b1615b110>\ntemp_dir = PosixPath('/tmp/tmpu1ns06wz')\n\n    def test_validate_directory_path_creation_error(self, temp_dir):\n        \"\"\"Test directory creation error handling.\"\"\"\n        # Try to create directory with invalid name\n        invalid_dir = temp_dir / \"invalid\\x00name\"\n    \n>       result = ValidationUtils.validate_directory_path(\n            invalid_dir, must_exist=True, create_if_missing=True\n        )\n\ntests/unit/test_utils/test_validation.py:512: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \nsrc/retileup/utils/validation.py:497: in validate_directory_path\n    path.mkdir(parents=True, exist_ok=True)\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = PosixPath('/tmp/tmpu1ns06wz/invalid\\x00name'), mode = 511, parents = True\nexist_ok = True\n\n    def mkdir(self, mode=0o777, parents=False, exist_ok=False):\n        \"\"\"\n        Create a new directory at this given path.\n        \"\"\"\n        try:\n>           os.mkdir(self, mode)\nE           ValueError: embedded null byte\n\n/usr/lib/python3.11/pathlib.py:1117: ValueError\n=============================== warnings summary ===============================\ntests/performance/test_performance.py:32\n  /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/tests/performance/test_performance.py:32: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n    pytest.mark.performance,\n\ntests/unit/test_core/test_config.py::TestConfig::test_save_to_file\n  /home/olereon/workspace/github.com/olereon/0_CC_prjcts/ReTileUp/src/retileup/core/config.py:162: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n    config_dict = self.dict()\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n================================ tests coverage ================================\n_____________ coverage: platform linux, python 3.11.0-candidate-1 ______________\n\nName                                    Stmts   Miss Branch BrPart   Cover   Missing\n------------------------------------------------------------------------------------\nsrc/retileup/__main__.py                   12     12      0      0   0.00%   3-17\nsrc/retileup/schemas/__init__.py            3      3      0      0   0.00%   3-6\nsrc/retileup/schemas/config.py            103    103     10      0   0.00%   8-271\nsrc/retileup/schemas/workflow.py          205    205     40      0   0.00%   3-365\nsrc/retileup/cli/commands/utils.py        220    200    110      0   6.06%   20-21, 26-27, 40-69, 82-93, 106-117, 158-235, 247-277, 289-336, 390-491\nsrc/retileup/cli/commands/workflow.py     201    183     82      0   6.36%   22-45, 61-84, 100-120, 135-158, 235-447\nsrc/retileup/cli/completion.py            116    106     40      0   6.41%   21-26, 39-67, 79-80, 92-98, 138-254\nsrc/retileup/cli/commands/tile.py         132    119     58      0   6.84%   29-54, 67-74, 165-328\nsrc/retileup/core/orchestrator.py         126    101     34      0  15.62%   44-48, 65-67, 72-80, 98-141, 162-192, 213-249, 271-303, 318-321, 325-327\nsrc/retileup/utils/progress.py            140    109     58      0  15.66%   26-28, 40-42, 64-88, 103-115, 127-130, 143-145, 153, 170-181, 189, 193-195, 203, 216-219, 240-250, 274-295, 306-317, 331-337, 349-371, 395-404, 423-425\nsrc/retileup/core/workflow.py             107     47     24      0  45.80%   52-54, 58-60, 92-94, 100-108, 131-141, 152-155, 166-170, 174, 185, 196, 200-201, 212-224, 232-242, 253, 258\nsrc/retileup/tools/tiling.py              239     49     78     19  77.29%   68, 73, 77, 80, 99, 102-103, 112, 117, 185-186, 202->213, 206-210, 241, 253-254, 296-297, 310->304, 323, 356-364, 410-412, 437-439, 459-470, 494-496, 504-505, 513-515\nsrc/retileup/core/config.py               104     15     40     13  77.78%   92->96, 97->99, 100, 101->105, 106->108, 109, 111, 112->116, 118, 120, 122, 124, 126, 133, 144-154\nsrc/retileup/utils/image.py               180     21     54      5  86.32%   56, 70, 76->81, 188-189, 194-197, 200, 503-532\nsrc/retileup/cli/main.py                   95      9     36      3  87.79%   60-63, 162-166, 220, 236->238, 240->244\nsrc/retileup/tools/base.py                 65      3     12      4  90.91%   68, 122, 130, 327->330\nsrc/retileup/utils/validation.py          302     25    138     13  90.91%   191-194, 199, 230->234, 238->243, 240-241, 245-246, 262-263, 277-278, 306-307, 336-337, 494, 495->503, 499, 501, 533, 536, 564, 689->695, 757-760\nsrc/retileup/core/registry.py             235     13     68      8  93.07%   117->121, 129, 235, 302-303, 397-398, 401->exit, 406, 456-457, 478->481, 482-483, 567-568\nsrc/retileup/__init__.py                    8      0      0      0 100.00%\nsrc/retileup/cli/__init__.py                1      0      0      0 100.00%\nsrc/retileup/cli/commands/__init__.py       4      0      0      0 100.00%\nsrc/retileup/core/__init__.py               5      0      0      0 100.00%\nsrc/retileup/core/exceptions.py           141      0     38      0 100.00%\nsrc/retileup/tools/__init__.py              3      0      0      0 100.00%\nsrc/retileup/utils/__init__.py              4      0      0      0 100.00%\n------------------------------------------------------------------------------------\nTOTAL                                    2751   1323    920     65  49.22%\nCoverage HTML written to dir htmlcov\nCoverage XML written to file coverage.xml\nCoverage JSON written to file coverage.json\nFAIL Required test coverage of 90% not reached. Total coverage: 49.22%\n============================= slowest 10 durations =============================\n1.08s call     tests/unit/test_tools/test_base.py::TestToolEdgeCases::test_tool_memory_efficiency\n0.83s call     tests/integration/test_tiling_integration.py::TestTilingIntegration::test_large_image_tiling\n0.27s call     tests/edge_cases/test_edge_cases.py::TestResourceExhaustion::test_disk_space_simulation\n0.20s call     tests/performance/test_performance.py::TestScalabilityLimits::test_stress_concurrent_operations\n0.19s setup    tests/integration/test_tiling_integration.py::TestTilingIntegration::test_concurrent_safety\n0.19s setup    tests/integration/test_tiling_integration.py::TestTilingIntegration::test_end_to_end_workflow\n0.19s call     tests/performance/test_performance.py::TestResourceUtilization::test_memory_usage_patterns\n0.16s call     tests/performance/test_performance.py::TestImageProcessingPerformance::test_image_resize_performance\n0.13s call     tests/edge_cases/test_edge_cases.py::TestRaceConditionsAndTiming::test_resource_cleanup_timing\n0.11s call     tests/performance/test_performance.py::TestImageProcessingPerformance::test_large_image_handling\n=========================== short test summary info ============================\nFAILED tests/edge_cases/test_edge_cases.py::TestCorruptedDataHandling::test_malformed_configuration_data\nFAILED tests/edge_cases/test_edge_cases.py::TestCorruptedDataHandling::test_invalid_coordinate_data\nFAILED tests/edge_cases/test_edge_cases.py::TestExtremeInputValues::test_extremely_large_numbers\nFAILED tests/integration/test_cli.py::TestCLIBasics::test_cli_no_args_shows_help\nFAILED tests/integration/test_cli.py::TestCLIConfiguration::test_global_state_initialization\nFAILED tests/integration/test_cli.py::TestCLIErrorHandling::test_handle_retileup_error\nFAILED tests/integration/test_cli.py::TestCLIIntegration::test_cli_callback_order\nFAILED tests/integration/test_cli.py::TestCLIIntegration::test_main_execution_path\nFAILED tests/integration/test_cli.py::TestCLIPerformance::test_cli_concurrent_execution\nFAILED tests/integration/test_cli.py::TestCLIEdgeCases::test_cli_signal_handling\nFAILED tests/integration/test_end_to_end.py::TestEndToEndImageProcessing::test_image_processing_memory_efficiency\nFAILED tests/integration/test_tiling_integration.py::TestTilingIntegration::test_large_image_tiling\nFAILED tests/integration/test_tiling_integration.py::TestTilingIntegration::test_concurrent_safety\nFAILED tests/performance/test_performance.py::TestImageProcessingPerformance::test_large_image_handling\nFAILED tests/unit/test_core/test_exceptions.py::TestErrorCode::test_error_code_string_comparison\nFAILED tests/unit/test_core/test_exceptions.py::TestErrorChaining::test_nested_error_chain\nFAILED tests/unit/test_core/test_exceptions.py::TestErrorChaining::test_error_serialization_with_cause\nFAILED tests/unit/test_core/test_registry.py::TestToolRegistry::test_tool_registration_failing_tool\nFAILED tests/unit/test_core/test_registry.py::TestToolRegistry::test_create_tool_instance_failure\nFAILED tests/unit/test_core/test_registry.py::TestToolRegistryHealthAndValidation::test_validate_tool_health_failure\nFAILED tests/unit/test_core/test_registry.py::TestToolRegistryThreadSafety::test_concurrent_access\nFAILED tests/unit/test_tools/test_base.py::TestToolEdgeCases::test_tool_execution_with_none_config\nFAILED tests/unit/test_tools/test_base.py::TestToolEdgeCases::test_tool_memory_efficiency\nFAILED tests/unit/test_tools/test_tiling.py::TestTilingConfig::test_invalid_tile_dimensions\nFAILED tests/unit/test_tools/test_tiling.py::TestTilingConfig::test_invalid_coordinates\nFAILED tests/unit/test_tools/test_tiling.py::TestTilingToolFormats::test_rgba_to_rgb_conversion\nFAILED tests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_file_path_factory_permissions\nFAILED tests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_image_file_factory_error_handling\nFAILED tests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_directory_path_creation_error\nERROR tests/integration/test_workflows.py::TestWorkflowBasics::test_workflow_tool_registration\nERROR tests/integration/test_workflows.py::TestWorkflowBasics::test_workflow_tool_retrieval\nERROR tests/integration/test_workflows.py::TestWorkflowExecution::test_simple_workflow_execution\nERROR tests/integration/test_workflows.py::TestWorkflowExecution::test_workflow_with_output_files\nERROR tests/integration/test_workflows.py::TestWorkflowExecution::test_workflow_execution_failure\nERROR tests/integration/test_workflows.py::TestWorkflowExecution::test_workflow_execution_with_timing\nERROR tests/integration/test_workflows.py::TestWorkflowExecution::test_workflow_multiple_executions\nERROR tests/integration/test_workflows.py::TestWorkflowExecution::test_workflow_parallel_configuration\nERROR tests/integration/test_workflows.py::TestWorkflowConcurrency::test_concurrent_workflow_execution\nERROR tests/integration/test_workflows.py::TestWorkflowConcurrency::test_workflow_thread_safety\nERROR tests/integration/test_workflows.py::TestWorkflowConcurrency::test_workflow_resource_contention\nERROR tests/integration/test_workflows.py::TestWorkflowErrorHandling::test_workflow_validation_error_handling\nERROR tests/integration/test_workflows.py::TestWorkflowErrorHandling::test_workflow_processing_error_recovery\nERROR tests/integration/test_workflows.py::TestWorkflowErrorHandling::test_workflow_timeout_handling\nERROR tests/integration/test_workflows.py::TestWorkflowErrorHandling::test_workflow_partial_failure_handling\nERROR tests/integration/test_workflows.py::TestWorkflowErrorHandling::test_workflow_cleanup_on_failure\nERROR tests/integration/test_workflows.py::TestWorkflowPerformance::test_workflow_execution_time\nERROR tests/integration/test_workflows.py::TestWorkflowPerformance::test_workflow_memory_efficiency\nERROR tests/integration/test_workflows.py::TestWorkflowPerformance::test_workflow_scalability\nERROR tests/integration/test_workflows.py::TestWorkflowPerformance::test_workflow_resource_cleanup\nERROR tests/integration/test_workflows.py::TestWorkflowIntegration::test_end_to_end_workflow\nERROR tests/integration/test_workflows.py::TestWorkflowIntegration::test_workflow_chaining\nERROR tests/integration/test_workflows.py::TestWorkflowIntegration::test_workflow_error_propagation\nERROR tests/integration/test_workflows.py::TestWorkflowIntegration::test_workflow_configuration_inheritance\nERROR tests/integration/test_workflows.py::TestWorkflowIntegration::test_workflow_state_management\nERROR tests/unit/test_utils/test_image.py::TestImageLoading::test_load_image_success\nERROR tests/unit/test_utils/test_image.py::TestImageLoading::test_load_image_with_conversion\nERROR tests/unit/test_utils/test_image.py::TestImageLoading::test_load_image_with_exif\nERROR tests/unit/test_utils/test_image.py::TestImageInfo::test_get_image_info_with_file_size\nERROR tests/unit/test_utils/test_image.py::TestImageInfo::test_get_image_info_with_exif\nERROR tests/unit/test_utils/test_image.py::TestImageValidation::test_is_valid_image_success\nERROR tests/unit/test_utils/test_image.py::TestImageValidation::test_validate_image_format_success\nERROR tests/unit/test_utils/test_image.py::TestTileInfo::test_create_tile_info_success\nERROR tests/unit/test_utils/test_image.py::TestTileInfo::test_create_tile_info_with_overlap\nERROR tests/unit/test_utils/test_image.py::TestTileInfo::test_create_tile_info_invalid_tiles\nERROR tests/unit/test_utils/test_image.py::TestTileInfo::test_create_tile_info_memory_feasibility\nERROR tests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_thread_safety_image_operations\nERROR tests/unit/test_utils/test_image.py::TestImageUtilsEdgeCases::test_memory_efficiency_large_operations\nERROR tests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_file_path_factory_existing_file\nERROR tests/unit/test_utils/test_validation.py::TestFileValidation::test_validate_image_file_factory_success\nERROR tests/unit/test_utils/test_validation.py::TestValidationUtils::test_validate_file_path_success\nERROR tests/unit/test_utils/test_validation.py::TestValidationEdgeCases::test_path_validation_with_string_input\n======= 29 failed, 416 passed, 4 skipped, 2 warnings, 42 errors in 9.24s =======\n",
    "stderr": "/home/olereon/.local/lib/python3.11/site-packages/coverage/html.py:131: CoverageWarning: No contexts were measured\n  self.coverage._warn(\"No contexts were measured\")\n",
    "duration": 8.748866081237793,
    "description": "Full test suite"
  }
}